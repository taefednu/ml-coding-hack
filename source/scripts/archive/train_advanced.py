"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π ML Pipeline –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–µ—Ñ–æ–ª—Ç–∞ –∑–∞–µ–º—â–∏–∫–∞.
–§–æ–∫—É—Å –Ω–∞ –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—é PR-AUC –¥–æ 80%.

–£–ª—É—á—à–µ–Ω–∏—è:
1. –ì–ª—É–±–æ–∫–∏–π feature engineering
2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (RandomizedSearchCV)
3. SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤
4. –ê–Ω—Å–∞–º–±–ª–∏ –º–æ–¥–µ–ª–µ–π (Voting, Stacking)
5. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ PR-AUC
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
import pickle
import warnings
import time
from datetime import datetime, timedelta
from scipy.stats import spearmanr
warnings.filterwarnings('ignore')

# –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("‚ö† tqdm –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tqdm")

from typing import Tuple
from sklearn.model_selection import (
    StratifiedKFold, 
    train_test_split,
    RandomizedSearchCV
)
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (
    RandomForestClassifier, 
    VotingClassifier,
    GradientBoostingClassifier
)
from sklearn.metrics import (
    roc_auc_score, 
    average_precision_score, 
    make_scorer,
    classification_report,
    confusion_matrix
)
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN
from imblearn.combine import SMOTETomek, SMOTEENN
from imblearn.pipeline import Pipeline as ImbPipeline

# Gradient Boosting
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except (ImportError, Exception) as e:
    XGBOOST_AVAILABLE = False
    print(f"‚ö† XGBoost –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {type(e).__name__}")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except (ImportError, Exception) as e:
    LIGHTGBM_AVAILABLE = False
    print(f"‚ö† LightGBM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {type(e).__name__}")

try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except (ImportError, Exception) as e:
    CATBOOST_AVAILABLE = False
    print(f"‚ö† CatBoost –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {type(e).__name__}")

try:
    import optuna
    OPTUNA_AVAILABLE = True
except (ImportError, Exception):
    OPTUNA_AVAILABLE = False
    print("‚ö† Optuna –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º RandomizedSearchCV")

import prepare_data as prep


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

BASE_PATH = Path("data_sets")
RANDOM_STATE = 42
N_SPLITS = 5
EXCLUDE_COLS = ["customer_id", "default", "application_id"]  # application_id –º–æ–∂–µ—Ç –±—ã—Ç—å —à—É–º–æ–º
MAX_JOBS = int(os.environ.get("MAX_JOBS", "1"))
CATBOOST_THREADS = int(os.environ.get("CATBOOST_THREADS", str(MAX_JOBS)))

# –°—Ç—Ä–æ–∫–æ–≤—ã–π —Å–∫–æ—É–µ—Ä –±–µ–∑–æ–ø–∞—Å–µ–Ω –¥–∞–∂–µ –≤ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö sklearn
PR_AUC_SCORER = "average_precision"

# –í–µ—Å–∞ –∞–Ω—Å–∞–º–±–ª—è: 1.0 = —Ç–æ–ª—å–∫–æ PR-AUC, 0.0 = —Ç–æ–ª—å–∫–æ Spearman, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–∏–ª—å–Ω—ã–π —Ñ–æ–∫—É—Å –Ω–∞ PR-AUC
ENSEMBLE_ALPHA = 0.7


# ============================================================================
# –£–õ–£–ß–®–ï–ù–ù–´–ô FEATURE ENGINEERING
# ============================================================================

def advanced_feature_engineering(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ì–ª—É–±–æ–∫–∏–π feature engineering:
    - –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    - –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    """
    df = df.copy()
    
    print("   üîß –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    def add_woe_encoding(column_name: str, smoothing: float = 5.0):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç WOE-—ç–Ω–∫–æ–¥–∏–Ω–≥ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å –ª–µ–≥–∫–æ–π
        —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π (—Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ), —á—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å —à—É–º –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö.
        """
        if column_name not in df.columns or "default" not in df.columns:
            return
        temp = df[[column_name, "default"]].dropna()
        if temp.empty:
            return
        stats = temp.groupby(column_name)["default"].agg(["sum", "count"])
        stats["non_default"] = stats["count"] - stats["sum"]
        total_pos = stats["sum"].sum()
        total_neg = stats["non_default"].sum()
        if total_pos == 0 or total_neg == 0:
            return
        n_unique = stats.shape[0]
        stats["pos_dist"] = (stats["sum"] + smoothing) / (total_pos + smoothing * n_unique)
        stats["neg_dist"] = (stats["non_default"] + smoothing) / (total_neg + smoothing * n_unique)
        stats["woe"] = np.log(stats["pos_dist"] / stats["neg_dist"])
        df[f"{column_name}_woe"] = df[column_name].map(stats["woe"]).fillna(0)

    def add_group_target_rate(column_name: str, new_col: str, smoothing: float = 5.0):
        """–°–≥–ª–∞–∂–µ–Ω–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–µ—Ñ–æ–ª—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (target encoding)."""
        if column_name not in df.columns or "default" not in df.columns:
            return
        temp = df[[column_name, "default"]].dropna()
        if temp.empty:
            return
        stats = temp.groupby(column_name)["default"].agg(["sum", "count"])
        global_rate = temp["default"].mean()
        stats[new_col] = (stats["sum"] + smoothing * global_rate) / (stats["count"] + smoothing)
        df[new_col] = df[column_name].map(stats[new_col]).fillna(global_rate)
    
    # 1. –ë–∞–∑–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
    if "monthly_income" in df.columns:
        if "total_monthly_debt_payment" in df.columns:
            df["debt_to_income_ratio"] = (
                df["total_monthly_debt_payment"] / (df["monthly_income"] + 1e-6)
            )
            df["debt_to_income_ratio_squared"] = df["debt_to_income_ratio"] ** 2
        
        if "monthly_free_cash_flow" in df.columns:
            df["cash_flow_ratio"] = (
                df["monthly_free_cash_flow"] / (df["monthly_income"] + 1e-6)
            )
            df["cash_surplus_ratio"] = (
                (df["monthly_income"] - df["total_monthly_debt_payment"].fillna(0))
                / (df["monthly_income"] + 1e-6)
                if "total_monthly_debt_payment" in df.columns else np.nan
            )
            if "total_monthly_debt_payment" in df.columns:
                df["cash_to_debt_ratio"] = (
                    (df["monthly_free_cash_flow"] + 1) / (df["total_monthly_debt_payment"] + 1)
                )
                df["free_cashflow_stress_flag"] = (
                    (df["cash_surplus_ratio"] < -0.1).astype(int)
                )
        
        if "loan_amount" in df.columns:
            df["loan_to_income_ratio"] = (
                df["loan_amount"] / (df["monthly_income"] * 12 + 1e-6)
            )
            if "total_credit_limit" in df.columns:
                df["loan_to_credit_limit_ratio"] = (
                    df["loan_amount"] / (df["total_credit_limit"] + 1e-6)
                )
            if "available_credit" in df.columns:
                df["loan_to_available_credit"] = (
                    df["loan_amount"] / (df["available_credit"] + 1e-6)
                )
    
    # 2. –ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    if "credit_score" in df.columns:
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∞
        df["credit_score_normalized"] = (df["credit_score"] - 300) / (850 - 300)
        
        # –ë–∏–Ω–Ω–∏–Ω–≥ –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∞
        df["credit_score_bin"] = pd.cut(
            df["credit_score"],
            bins=[0, 580, 670, 740, 850],
            labels=[0, 1, 2, 3]  # poor, fair, good, excellent
        ).astype(float).fillna(0)  # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –Ω—É–ª–µ–º
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∞ —Å –¥–æ—Ö–æ–¥–æ–º (–í–´–°–û–ö–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢)
        if "annual_income" in df.columns:
            df["credit_score_income_interaction"] = (
                df["credit_score_normalized"] * np.log1p(df["annual_income"])
            )
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∞ —Å –¥–æ–ª–≥–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–æ–π (–ö–õ–Æ–ß–ï–í–û–ï!)
        if "debt_to_income_ratio" in df.columns:
            df["credit_score_debt_interaction"] = (
                (1 - df["credit_score_normalized"]) * df["debt_to_income_ratio"]
            )
            df["credit_stress_ratio"] = (
                df["debt_to_income_ratio"] * (1 + df["credit_score_normalized"])
            )
    
    # 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–∞
    if "credit_usage_amount" in df.columns and "available_credit" in df.columns:
        df["credit_utilization"] = (
            df["credit_usage_amount"] / (df["available_credit"] + df["credit_usage_amount"] + 1e-6)
        )
        
        # –ö–≤–∞–¥—Ä–∞—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ (–Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å)
        df["credit_utilization_squared"] = df["credit_utilization"] ** 2
        
        # –ë–∏–Ω–Ω–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞
        df["credit_utilization_bin"] = pd.cut(
            df["credit_utilization"],
            bins=[0, 0.3, 0.5, 0.7, 1.0],
            labels=[0, 1, 2, 3]  # low, medium, high, very_high
        ).astype(float).fillna(0)  # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –Ω—É–ª–µ–º
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–∞ + –ø—Ä–æ—Å—Ä–æ—á–∫–∏ (–ö–õ–Æ–ß–ï–í–û–ï!)
        if "num_delinquencies_2yrs" in df.columns:
            df["utilization_delinquency_interaction"] = (
                df["credit_utilization"] * df["num_delinquencies_2yrs"].fillna(0)
            )
        if "credit_score_normalized" in df.columns:
            df["score_utilization_interaction"] = (
                df["credit_utilization"] * (1 - df["credit_score_normalized"])
            )
    
    # 4. –î–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
    if "total_debt_amount" in df.columns and "annual_income" in df.columns:
        df["debt_to_annual_income"] = (
            df["total_debt_amount"] / (df["annual_income"] + 1e-6)
        )
    
    if "debt_to_income_ratio" in df.columns and "credit_utilization" in df.columns:
        df["combined_risk_ratio"] = df["debt_to_income_ratio"] * (df["credit_utilization"] + 1)
    
    if "debt_to_income_ratio" in df.columns:
        df["debt_service_ratio"] = df["debt_to_income_ratio"] * df.get("loan_to_income_ratio", 1)
        df["dti_high_flag"] = (df["debt_to_income_ratio"] > 0.6).astype(int)
        df["dti_extreme_flag"] = (df["debt_to_income_ratio"] > 0.85).astype(int)
        try:
            df["dti_quantile_bin"] = pd.qcut(
                df["debt_to_income_ratio"].clip(0, 5), q=4, labels=[0, 1, 2, 3]
            ).astype(float)
        except ValueError:
            df["dti_quantile_bin"] = 0.0
    
    # 5. –í–æ–∑—Ä–∞—Å—Ç –∏ —Å—Ç–∞–∂
    if "age" in df.columns:
        # –ö–≤–∞–¥—Ä–∞—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞ (–Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å)
        df["age_squared"] = df["age"] ** 2
        
        # –ë–∏–Ω–Ω–∏–Ω–≥ –≤–æ–∑—Ä–∞—Å—Ç–∞
        df["age_bin"] = pd.cut(
            df["age"],
            bins=[0, 25, 35, 45, 55, 100],
            labels=[0, 1, 2, 3, 4]  # young, adult, middle, senior, elderly
        ).astype(float).fillna(1)  # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (adult)
        
        if "employment_length" in df.columns:
            df["employment_to_age_ratio"] = (
                df["employment_length"] / (df["age"] + 1e-6)
            )
            
            # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ —Å—Ç–∞–∂–∞ (–º–æ–ª–æ–¥–æ–π + –∫–æ—Ä–æ—Ç–∫–∏–π —Å—Ç–∞–∂ = —Ä–∏—Å–∫)
            df["age_employment_interaction"] = (
                (df["age"] < 30).astype(int) * (df["employment_length"] < 2).astype(int)
            )
    
    # 6. –ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è
    if "num_credit_accounts" in df.columns:
        if "oldest_credit_line_age" in df.columns:
            df["credit_accounts_per_year"] = (
                df["num_credit_accounts"] / (df["oldest_credit_line_age"] + 1e-6)
            )
        
        # –ü—Ä–æ–±–ª–µ–º—ã —Å –∫—Ä–µ–¥–∏—Ç–∞–º–∏
        if "num_delinquencies_2yrs" in df.columns:
            df["delinquency_rate"] = (
                df["num_delinquencies_2yrs"] / (df["num_credit_accounts"] + 1e-6)
            )
        
        if "num_collections" in df.columns:
            df["collection_rate"] = (
                df["num_collections"] / (df["num_credit_accounts"] + 1e-6)
            )
    
    # 7. –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    if "regional_median_income" in df.columns and "annual_income" in df.columns:
        df["income_vs_regional"] = (
            df["annual_income"] / (df["regional_median_income"] + 1e-6)
        )
        df["income_gap_vs_region"] = (
            df["annual_income"] - df["regional_median_income"]
        )
    if "regional_unemployment_rate" in df.columns and "unemployment_squared" not in df.columns:
        df["unemployment_squared"] = df["regional_unemployment_rate"] ** 2
    
    if "regional_unemployment_rate" in df.columns:
        # –ö–≤–∞–¥—Ä–∞—Ç –±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü—ã
        df["unemployment_squared"] = df["regional_unemployment_rate"] ** 2
    
    # 8. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    if "application_hour" in df.columns:
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–∏
        df["hour_sin"] = np.sin(2 * np.pi * df["application_hour"] / 24)
        df["hour_cos"] = np.cos(2 * np.pi * df["application_hour"] / 24)
    
    if "application_day_of_week" in df.columns:
        df["day_sin"] = np.sin(2 * np.pi * df["application_day_of_week"] / 7)
        df["day_cos"] = np.cos(2 * np.pi * df["application_day_of_week"] / 7)
    
    if "account_open_year" in df.columns:
        # –°–∫–æ–ª—å–∫–æ –ª–µ—Ç –Ω–∞–∑–∞–¥ –æ—Ç–∫—Ä—ã—Ç —Å—á–µ—Ç
        current_year = df["account_open_year"].max()
        df["account_age_years"] = current_year - df["account_open_year"]
        if "num_login_sessions" in df.columns:
            df["sessions_per_year"] = (
                df["num_login_sessions"] / (df["account_age_years"] + 1)
            )
        if "loan_amount" in df.columns:
            df["loan_per_account_age"] = df["loan_amount"] / (df["account_age_years"] + 1)
    
    # 9. –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã —Ä–∏—Å–∫–∞
    if all(col in df.columns for col in ["credit_score_normalized", "debt_to_income_ratio", 
                                         "credit_utilization", "delinquency_rate"]):
        df["risk_index"] = (
            (1 - df["credit_score_normalized"]) * 0.3 +
            df["debt_to_income_ratio"].clip(0, 2) * 0.3 +
            df["credit_utilization"] * 0.2 +
            df["delinquency_rate"].fillna(0) * 0.2
        )
    
    # 10. –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è skewed –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    skewed_cols = ["annual_income", "monthly_income", "loan_amount", "total_debt_amount"]
    for col in skewed_cols:
        if col in df.columns:
            df[f"{col}_log"] = np.log1p(df[col])
    
    # 11. –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    if all(col in df.columns for col in ["num_login_sessions", "num_customer_service_calls"]):
        df["service_call_ratio"] = (
            df["num_customer_service_calls"] / (df["num_login_sessions"] + 1)
        )
        median_sessions = df["num_login_sessions"].median()
        median_calls = df["num_customer_service_calls"].median()
        df["low_activity_high_calls"] = (
            (df["num_login_sessions"] < median_sessions) &
            (df["num_customer_service_calls"] > median_calls)
        ).astype(int)
    
    # 12. WOE-—ç–Ω–∫–æ–¥–∏–Ω–≥ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    categorical_for_woe = [
        "employment_type",
        "preferred_contact",
        "state",
        "region",
        "account_status_code",
        "referral_code"
    ]
    for cat_col in categorical_for_woe:
        add_woe_encoding(cat_col, smoothing=10.0)

    target_rate_cols = {
        "state": "state_default_rate",
        "region": "region_default_rate",
        "employment_type": "employment_default_rate",
        "referral_code": "referral_default_rate"
    }
    for col, new_col in target_rate_cols.items():
        add_group_target_rate(col, new_col, smoothing=20.0)
    
    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(df.columns)}")
    
    return df


# ============================================================================
# –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•
# ============================================================================

def handle_missing_values_advanced(df: pd.DataFrame, target_col: str = "default") -> pd.DataFrame:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤."""
    df = df.copy()
    
    for col in df.columns:
        if col in EXCLUDE_COLS:
            continue
            
        if df[col].isna().sum() == 0:
            continue
            
        if pd.api.types.is_numeric_dtype(df[col]):
            # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö: –º–µ–¥–∏–∞–Ω–∞, –Ω–æ —Å —É—á–µ—Ç–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            median_val = df[col].median()
            if pd.isna(median_val):
                median_val = 0
            df[col] = df[col].fillna(median_val)
        else:
            # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: –º–æ–¥–∞ –∏–ª–∏ "unknown"
            mode_val = df[col].mode()
            if len(mode_val) > 0:
                df[col] = df[col].fillna(mode_val[0])
            else:
                df[col] = df[col].fillna("unknown")
    
    return df


def encode_categorical_advanced(df: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:
    """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
    df = df.copy()
    encoders = {}
    
    for col in df.columns:
        if col in EXCLUDE_COLS:
            continue
            
        if df[col].dtype == "object" or df[col].dtype.name == "category":
            le = LabelEncoder()
            df[col] = df[col].fillna("unknown")
            df[col] = le.fit_transform(df[col].astype(str))
            encoders[col] = le
    
    return df, encoders


def prepare_features_advanced(df: pd.DataFrame, target_col: str = "default") -> Tuple[pd.DataFrame, pd.Series, list]:
    """–ü–æ–ª–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º feature engineering."""
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    df = handle_missing_values_advanced(df, target_col)
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π feature engineering
    df = advanced_feature_engineering(df)
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    df, encoders = encode_categorical_advanced(df)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    feature_cols = [c for c in df.columns if c not in EXCLUDE_COLS]
    X = df[feature_cols].copy()
    y = df[target_col].copy()
    
    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ
    for col in X.columns:
        if not pd.api.types.is_numeric_dtype(X[col]):
            X[col] = pd.to_numeric(X[col], errors="coerce").fillna(0)
    
    # –ó–∞–º–µ–Ω–∞ inf –∏ –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(X.median())
    
    return X, y, feature_cols


def balance_with_smoteenn(X: pd.DataFrame, y: pd.Series):
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç SMOTEENN –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞."""
    try:
        sampler = SMOTEENN(random_state=RANDOM_STATE)
        X_res, y_res = sampler.fit_resample(X, y)
        return pd.DataFrame(X_res, columns=X.columns), pd.Series(y_res)
    except Exception as exc:
        print(f"      ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å SMOTEENN: {exc}")
        return X, y


def balance_with_smote_tomek(X: pd.DataFrame, y: pd.Series):
    """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç SMOTE-Tomek –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""
    try:
        sampler = SMOTETomek(random_state=RANDOM_STATE)
        X_res, y_res = sampler.fit_resample(X, y)
        return pd.DataFrame(X_res, columns=X.columns), pd.Series(y_res)
    except Exception as exc:
        print(f"      ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å SMOTETomek: {exc}. –í–æ–∑–≤—Ä–∞—â–∞–µ–º SMOTEENN.")
        return balance_with_smoteenn(X, y)


# ============================================================================
# –ú–û–î–ï–õ–ò –° –ù–ê–°–¢–†–û–ô–ö–û–ô –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í
# ============================================================================

def train_tuned_catboost(X_train: pd.DataFrame, y_train: pd.Series,
                        X_val: pd.DataFrame, y_val: pd.Series):
    """CatBoost —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π."""
    if not CATBOOST_AVAILABLE:
        return None
    
    print("      üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CatBoost...")

    print("      üîÅ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ SMOTE-Tomek...")
    X_train_balanced, y_train_balanced = balance_with_smote_tomek(X_train, y_train)
    
    # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–ª—è PR-AUC
    base_weight = (y_train_balanced == 0).sum() / (y_train_balanced == 1).sum()

    if OPTUNA_AVAILABLE:
        try:
            best_model, study = tune_catboost_with_optuna(X_train_balanced, y_train_balanced)
            if best_model:
                y_pred_proba = best_model.predict_proba(X_val)[:, 1]
                y_pred = best_model.predict(X_val)
                auc = roc_auc_score(y_val, y_pred_proba)
                pr_auc = average_precision_score(y_val, y_pred_proba)
                spearman_corr, _ = spearmanr(y_val, y_pred_proba)
                return {
                    "model": best_model,
                    "scaler": None,
                    "auc": auc,
                    "pr_auc": pr_auc,
                    "spearman": spearman_corr,
                    "y_pred_proba": y_pred_proba,
                    "y_pred": y_pred,
                    "name": "CatBoost (Optuna Tuned)",
                    "best_params": study.best_params
                }
        except Exception as exc:
            print(f"      ‚ö† Optuna –Ω–µ —É–¥–∞–ª–∞—Å—å: {exc}. –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ RandomizedSearchCV.")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ PR-AUC (—É–º–µ—Ä–µ–Ω–Ω—ã–π –æ–±—ä–µ–º)
    param_distributions = {
        'iterations': [300, 500, 700],
        'depth': [6, 8, 10],
        'learning_rate': [0.05, 0.1],
        'l2_leaf_reg': [3, 5, 7],
        'scale_pos_weight': [base_weight, base_weight * 1.5, base_weight * 2],
        'min_data_in_leaf': [1, 3]
    }
    
    # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ PR-AUC –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
    base_model = CatBoostClassifier(
        random_state=RANDOM_STATE,
        verbose=100,  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 100 –∏—Ç–µ—Ä–∞—Ü–∏–π
        eval_metric='PRAUC',
        loss_function='Logloss',
        thread_count=CATBOOST_THREADS  # –£—á–∏—Ç—ã–≤–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è
    )
    
    # RandomizedSearchCV —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
    n_iter = 15
    cv_folds = 2
    total_fits = n_iter * cv_folds
    
    print(f"      üìä –í—Å–µ–≥–æ –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–æ: {total_fits} –º–æ–¥–µ–ª–µ–π")
    print(f"      ‚è± –ù–∞—á–∞–ª–æ: {datetime.now().strftime('%H:%M:%S')}")
    print(f"      üíª –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–µ—Å—É—Ä—Å—ã CPU (n_jobs={MAX_JOBS}, thread_count={CATBOOST_THREADS})")
    
    # RandomizedSearchCV (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: –º–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π, –º–µ–Ω—å—à–µ CV)
    random_search = RandomizedSearchCV(
        base_model,
        param_distributions,
        n_iter=n_iter,
        scoring=PR_AUC_SCORER,
        cv=cv_folds,
        n_jobs=MAX_JOBS,
        random_state=RANDOM_STATE,
        verbose=1  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å sklearn (–±—É–¥–µ—Ç –≤–∏–¥–Ω–æ –≤ –∫–æ–Ω—Å–æ–ª–∏)
    )
    
    start_time = time.time()
    print(f"      üîÑ –û–±—É—á–µ–Ω–∏–µ {total_fits} –º–æ–¥–µ–ª–µ–π (sklearn –ø–æ–∫–∞–∂–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å)...")
    random_search.fit(X_train_balanced, y_train_balanced)
    elapsed_time = time.time() - start_time
    hours = int(elapsed_time // 3600)
    mins = int((elapsed_time % 3600) // 60)
    secs = int(elapsed_time % 60)
    print(f"      ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞: {hours:02d}:{mins:02d}:{secs:02d} ({elapsed_time/60:.1f} –º–∏–Ω—É—Ç)")
    
    best_model = random_search.best_estimator_
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred_proba = best_model.predict_proba(X_val)[:, 1]
    y_pred = best_model.predict(X_val)
    
    auc = roc_auc_score(y_val, y_pred_proba)
    pr_auc = average_precision_score(y_val, y_pred_proba)
    spearman_corr, _ = spearmanr(y_val, y_pred_proba)
    
    print(f"      ‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {random_search.best_params_}")
    
    return {
        "model": best_model,
        "scaler": None,
        "auc": auc,
        "pr_auc": pr_auc,
        "spearman": spearman_corr,
        "y_pred_proba": y_pred_proba,
        "y_pred": y_pred,
        "name": "CatBoost (Tuned)",
        "best_params": random_search.best_params_
    }


def train_tuned_lightgbm(X_train: pd.DataFrame, y_train: pd.Series,
                         X_val: pd.DataFrame, y_val: pd.Series):
    """LightGBM —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    if not LIGHTGBM_AVAILABLE:
        return None
    
    print("      üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ LightGBM...")
    print("      üîÅ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ SMOTE-Tomek...")
    X_train_balanced, y_train_balanced = balance_with_smote_tomek(X_train, y_train)
    
    base_weight = (y_train_balanced == 0).sum() / (y_train_balanced == 1).sum()
    
    param_distributions = {
        'n_estimators': [300, 500, 700],
        'num_leaves': [31, 63, 127],
        'max_depth': [-1, 10, 15],
        'learning_rate': [0.03, 0.05, 0.1],
        'subsample': [0.7, 0.85, 1.0],
        'colsample_bytree': [0.7, 0.85, 1.0],
        'reg_alpha': [0, 1, 5],
        'reg_lambda': [0, 1, 5],
        'scale_pos_weight': [base_weight, base_weight * 1.5, base_weight * 2]
    }
    
    base_model = lgb.LGBMClassifier(
        objective='binary',
        boosting_type='gbdt',
        random_state=RANDOM_STATE,
        n_jobs=MAX_JOBS
    )
    
    n_iter = 12
    cv_folds = 2
    total_fits = n_iter * cv_folds
    
    print(f"      üìä –í—Å–µ–≥–æ –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–æ: {total_fits} –º–æ–¥–µ–ª–µ–π")
    print(f"      ‚è± –ù–∞—á–∞–ª–æ: {datetime.now().strftime('%H:%M:%S')}")
    
    random_search = RandomizedSearchCV(
        base_model,
        param_distributions,
        n_iter=n_iter,
        scoring=PR_AUC_SCORER,
        cv=cv_folds,
        n_jobs=MAX_JOBS,
        random_state=RANDOM_STATE,
        verbose=0
    )
    
    start_time = time.time()
    print(f"      üîÑ –û–±—É—á–µ–Ω–∏–µ...", end="", flush=True)
    random_search.fit(X_train_balanced, y_train_balanced)
    elapsed_time = time.time() - start_time
    print(f"\r      ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞: {elapsed_time/60:.1f} –º–∏–Ω—É—Ç")
    best_model = random_search.best_estimator_
    y_pred_proba = best_model.predict_proba(X_val)[:, 1]
    y_pred = best_model.predict(X_val)
    
    auc = roc_auc_score(y_val, y_pred_proba)
    pr_auc = average_precision_score(y_val, y_pred_proba)
    spearman_corr, _ = spearmanr(y_val, y_pred_proba)
    
    print(f"      ‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {random_search.best_params_}")
    
    return {
        "model": best_model,
        "scaler": None,
        "auc": auc,
        "pr_auc": pr_auc,
        "spearman": spearman_corr,
        "y_pred_proba": y_pred_proba,
        "y_pred": y_pred,
        "name": "LightGBM (Tuned)",
        "best_params": random_search.best_params_
    }


def train_tuned_random_forest(X_train: pd.DataFrame, y_train: pd.Series,
                              X_val: pd.DataFrame, y_val: pd.Series):
    """Random Forest —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    print("      üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Random Forest...")

    print("      üîÅ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ SMOTE-Tomek...")
    X_train_balanced, y_train_balanced = balance_with_smote_tomek(X_train, y_train)
    
    param_distributions = {
        'n_estimators': [300, 500, 700],
        'max_depth': [15, 20, 25, None],
        'min_samples_split': [5, 10, 15],
        'min_samples_leaf': [2, 4, 6],
        'max_features': ['sqrt', 'log2', None],
        'class_weight': ['balanced', 'balanced_subsample']
    }
    
    base_model = RandomForestClassifier(
        random_state=RANDOM_STATE, 
        n_jobs=MAX_JOBS,
        verbose=1 if TQDM_AVAILABLE else 0
    )
    
    n_iter = 8
    cv_folds = 2
    total_fits = n_iter * cv_folds
    
    print(f"      üìä –í—Å–µ–≥–æ –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–æ: {total_fits} –º–æ–¥–µ–ª–µ–π")
    print(f"      ‚è± –ù–∞—á–∞–ª–æ: {datetime.now().strftime('%H:%M:%S')}")
    print(f"      üíª –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–µ—Å—É—Ä—Å—ã CPU (n_jobs={MAX_JOBS})")
    
    random_search = RandomizedSearchCV(
        base_model,
        param_distributions,
        n_iter=n_iter,
        scoring=PR_AUC_SCORER,
        cv=cv_folds,
        n_jobs=MAX_JOBS,
        random_state=RANDOM_STATE,
        verbose=0
    )
    
    start_time = time.time()
    print(f"      üîÑ –û–±—É—á–µ–Ω–∏–µ...", end="", flush=True)
    random_search.fit(X_train_balanced, y_train_balanced)
    elapsed_time = time.time() - start_time
    print(f"\r      ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞: {elapsed_time/60:.1f} –º–∏–Ω—É—Ç")
    best_model = random_search.best_estimator_
    y_pred_proba = best_model.predict_proba(X_val)[:, 1]
    y_pred = best_model.predict(X_val)
    
    auc = roc_auc_score(y_val, y_pred_proba)
    pr_auc = average_precision_score(y_val, y_pred_proba)
    spearman_corr, _ = spearmanr(y_val, y_pred_proba)
    
    print(f"      ‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {random_search.best_params_}")
    
    return {
        "model": best_model,
        "scaler": None,
        "auc": auc,
        "pr_auc": pr_auc,
        "spearman": spearman_corr,
        "y_pred_proba": y_pred_proba,
        "y_pred": y_pred,
        "name": "Random Forest (Tuned)",
        "best_params": random_search.best_params_
    }


def train_tuned_logistic_regression(X_train: pd.DataFrame, y_train: pd.Series,
                                    X_val: pd.DataFrame, y_val: pd.Series, use_borderline=True):
    """Logistic Regression —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –∏ SMOTE/BorderlineSMOTE."""
    print("      üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Logistic Regression —Å SMOTE...")
    
    # BorderlineSMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ (—Ñ–æ–∫—É—Å –Ω–∞ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö)
    if use_borderline:
        try:
            smote = BorderlineSMOTE(random_state=RANDOM_STATE, k_neighbors=5)
            X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
            print("      ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω BorderlineSMOTE")
        except:
            smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=5)
            X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
            print("      ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω SMOTE (fallback)")
    else:
        smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=5)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
    scaler = RobustScaler()  # RobustScaler –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º
    X_train_scaled = scaler.fit_transform(X_train_balanced)
    X_val_scaled = scaler.transform(X_val)
    
    param_distributions = {
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear', 'lbfgs'],
        'class_weight': ['balanced', None]
    }
    
    base_model = LogisticRegression(
        random_state=RANDOM_STATE, 
        max_iter=2000,
        n_jobs=MAX_JOBS
    )
    
    n_iter = 5
    cv_folds = 2
    total_fits = n_iter * cv_folds
    
    print(f"      üìä –í—Å–µ–≥–æ –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–æ: {total_fits} –º–æ–¥–µ–ª–µ–π")
    print(f"      ‚è± –ù–∞—á–∞–ª–æ: {datetime.now().strftime('%H:%M:%S')}")
    print(f"      üíª –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–µ—Å—É—Ä—Å—ã CPU (n_jobs={MAX_JOBS})")
    
    random_search = RandomizedSearchCV(
        base_model,
        param_distributions,
        n_iter=n_iter,
        scoring=PR_AUC_SCORER,
        cv=cv_folds,
        n_jobs=MAX_JOBS,
        random_state=RANDOM_STATE,
        verbose=0
    )
    
    start_time = time.time()
    print(f"      üîÑ –û–±—É—á–µ–Ω–∏–µ...", end="", flush=True)
    random_search.fit(X_train_scaled, y_train_balanced)
    elapsed_time = time.time() - start_time
    print(f"\r      ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞: {elapsed_time/60:.1f} –º–∏–Ω—É—Ç")
    best_model = random_search.best_estimator_
    y_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]
    y_pred = best_model.predict(X_val_scaled)
    
    auc = roc_auc_score(y_val, y_pred_proba)
    pr_auc = average_precision_score(y_val, y_pred_proba)
    spearman_corr, _ = spearmanr(y_val, y_pred_proba)
    
    print(f"      ‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {random_search.best_params_}")
    
    return {
        "model": best_model,
        "scaler": scaler,
        "smote": smote,
        "auc": auc,
        "pr_auc": pr_auc,
        "spearman": spearman_corr,
        "y_pred_proba": y_pred_proba,
        "y_pred": y_pred,
        "name": "Logistic Regression (Tuned + SMOTE)",
        "best_params": random_search.best_params_
    }


# ============================================================================
# –ê–ù–°–ê–ú–ë–õ–¨ –ú–û–î–ï–õ–ï–ô
# ============================================================================

def create_ensemble(models_results: list, X_val: pd.DataFrame, y_val: pd.Series):
    """–°–æ–∑–¥–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å –∏–∑ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π."""
    if len(models_results) < 2:
        return None
    
    print("\n   üéØ –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π...")
    
    sorted_models = sorted(models_results, key=lambda x: x['pr_auc'], reverse=True)[:3]
    
    predictions = []
    model_names = []
    
    for model_result in sorted_models:
        model = model_result['model']
        scaler = model_result.get('scaler')
        
        if scaler:
            X_val_scaled = scaler.transform(X_val)
            pred = model.predict_proba(X_val_scaled)[:, 1]
        else:
            pred = model.predict_proba(X_val)[:, 1]
        
        predictions.append(pred)
        model_names.append(model_result['name'])
    
    pr_auc_values = np.array([m['pr_auc'] for m in sorted_models])
    spearman_values = np.array([m.get('spearman', 0.0) for m in sorted_models])
    # –ù–æ—Ä–º–∏—Ä—É–µ–º Spearman –≤ [0, 1] —á–µ—Ä–µ–∑ —Å–¥–≤–∏–≥, —á—Ç–æ–±—ã —Å–æ—á–µ—Ç–∞—Ç—å —Å PR-AUC
    spearman_norm = (spearman_values + 1.0) / 2.0
    combined_scores = ENSEMBLE_ALPHA * pr_auc_values + (1.0 - ENSEMBLE_ALPHA) * spearman_norm
    # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω—É–ª–µ–≤—ã–µ –∏–ª–∏ NaN
    if np.all(combined_scores <= 0) or np.all(~np.isfinite(combined_scores)):
        combined_scores = pr_auc_values
    weights = combined_scores / combined_scores.sum()
    
    ensemble_pred = np.average(predictions, axis=0, weights=weights)
    ensemble_pred_binary = (ensemble_pred >= 0.5).astype(int)
    
    auc = roc_auc_score(y_val, ensemble_pred)
    pr_auc = average_precision_score(y_val, ensemble_pred)
    spearman_corr, _ = spearmanr(y_val, ensemble_pred)
    
    return {
        "model": sorted_models,
        "weights": weights,
        "scaler": None,
        "auc": auc,
        "pr_auc": pr_auc,
        "spearman": spearman_corr,
        "y_pred_proba": ensemble_pred,
        "y_pred": ensemble_pred_binary,
        "name": f"Ensemble ({len(sorted_models)} models)",
        "model_names": model_names
    }


# ============================================================================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    pipeline_start_time = time.time()
    
    print("=" * 80)
    print("–£–õ–£–ß–®–ï–ù–ù–´–ô ML PIPELINE: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ—Ñ–æ–ª—Ç–∞ –∑–∞–µ–º—â–∏–∫–∞")
    print("–¶–µ–ª—å: PR-AUC >= 0.80")
    print("=" * 80)
    print(f"üöÄ –ù–∞—á–∞–ª–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üíª –†–µ–∂–∏–º: –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã (n_jobs={MAX_JOBS}, thread_count={CATBOOST_THREADS})")
    print("=" * 80)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\n[1/7] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    master_df = prep.build_master_dataset()
    print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {master_df.shape[0]} —Å—Ç—Ä–æ–∫, {master_df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
    
    if "default" not in master_df.columns:
        raise ValueError("‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'default' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    
    print(f"\n   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞:")
    print(f"   {master_df['default'].value_counts().to_dict()}")
    print(f"   –î–æ–ª—è –¥–µ—Ñ–æ–ª—Ç–æ–≤: {master_df['default'].mean():.2%}")
    
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π
    print("\n[2/7] –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π feature engineering)...")
    X, y, feature_cols = prepare_features_advanced(master_df)
    print(f"   ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"   –ü—Ä–æ–ø—É—Å–∫–∏ –≤ X: {X.isna().sum().sum()}")
    
    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
    print("\n[3/7] –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test...")
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.2, random_state=RANDOM_STATE, stratify=y_temp
    )
    print(f"   Train: {X_train.shape[0]} —Å—Ç—Ä–æ–∫")
    print(f"   Val:   {X_val.shape[0]} —Å—Ç—Ä–æ–∫")
    print(f"   Test:  {X_test.shape[0]} —Å—Ç—Ä–æ–∫")
    
    # 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π
    print("\n[4/7] –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
    print("   ‚ö† –≠—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è (–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)...")
    
    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏–π
    total_model_fits = 8 * 2 + 5 * 2  # RF + LR
    model_types_count = 2  # RF, LR
    if CATBOOST_AVAILABLE:
        total_model_fits += 15 * 2  # CatBoost
        model_types_count += 1
    if LIGHTGBM_AVAILABLE:
        total_model_fits += 12 * 2  # LightGBM
        model_types_count += 1
    
    print(f"   üìä –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {model_types_count} —Ç–∏–ø–∞(–æ–≤)")
    print(f"   üìà –í—Å–µ–≥–æ –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–æ: ~{total_model_fits} –º–æ–¥–µ–ª–µ–π (—Å —É—á–µ—Ç–æ–º CV)")
    print(f"   ‚è± –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {datetime.now().strftime('%H:%M:%S')}")
    print(f"   üíª –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–µ—Å—É—Ä—Å—ã CPU (n_jobs={MAX_JOBS}, thread_count={CATBOOST_THREADS})")
    print()
    
    # –û–±—â–µ–µ –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
    total_start_time = time.time()
    models = []
    model_times = {}
    completed_models = 0
    
    def print_progress():
        elapsed_total = time.time() - total_start_time
        avg_time = elapsed_total / max(completed_models, 1)
        remaining_models = max(model_types_count - completed_models, 0)
        estimated_remaining = avg_time * remaining_models
        progress_pct = (completed_models / model_types_count) * 100
        print(f"      üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {completed_models}/{model_types_count} –º–æ–¥–µ–ª–µ–π ({progress_pct:.0f}%) | ‚è≥ –û—Å—Ç–∞–ª–æ—Å—å: ~{estimated_remaining/60:.1f} –º–∏–Ω")

    # CatBoost
    if CATBOOST_AVAILABLE:
        print(f"   üöÄ [{completed_models+1}/{model_types_count}] –û–±—É—á–µ–Ω–∏–µ CatBoost (Tuned)...")
        model_start = time.time()
        try:
            cb_result = train_tuned_catboost(X_train, y_train, X_val, y_val)
            if cb_result:
                models.append(cb_result)
                model_times['CatBoost'] = time.time() - model_start
                completed_models += 1
                print(f"      ‚úÖ AUC: {cb_result['auc']:.4f}, PR-AUC: {cb_result['pr_auc']:.4f}, Spearman: {cb_result['spearman']:.4f} ({model_times['CatBoost']/60:.1f} –º–∏–Ω)")
                print_progress()
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")

    # LightGBM
    if LIGHTGBM_AVAILABLE:
        print(f"   üöÄ [{completed_models+1}/{model_types_count}] –û–±—É—á–µ–Ω–∏–µ LightGBM (Tuned)...")
        model_start = time.time()
        try:
            lgb_result = train_tuned_lightgbm(X_train, y_train, X_val, y_val)
            if lgb_result:
                models.append(lgb_result)
                model_times['LightGBM'] = time.time() - model_start
                completed_models += 1
                print(f"      ‚úÖ AUC: {lgb_result['auc']:.4f}, PR-AUC: {lgb_result['pr_auc']:.4f}, Spearman: {lgb_result['spearman']:.4f} ({model_times['LightGBM']/60:.1f} –º–∏–Ω)")
                print_progress()
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")
    
    # Random Forest
    print(f"   üöÄ [{completed_models+1}/{model_types_count}] –û–±—É—á–µ–Ω–∏–µ Random Forest (Tuned)...")
    model_start = time.time()
    try:
        rf_result = train_tuned_random_forest(X_train, y_train, X_val, y_val)
        if rf_result:
            models.append(rf_result)
            model_times['Random Forest'] = time.time() - model_start
            completed_models += 1
            print(f"      ‚úÖ AUC: {rf_result['auc']:.4f}, PR-AUC: {rf_result['pr_auc']:.4f}, Spearman: {rf_result['spearman']:.4f} ({model_times['Random Forest']/60:.1f} –º–∏–Ω)")
            print_progress()
    except Exception as e:
        print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")
    
    # Logistic Regression —Å SMOTE
    print(f"   üöÄ [{completed_models+1}/{model_types_count}] –û–±—É—á–µ–Ω–∏–µ Logistic Regression (Tuned + SMOTE)...")
    model_start = time.time()
    try:
        lr_result = train_tuned_logistic_regression(X_train, y_train, X_val, y_val)
        if lr_result:
            models.append(lr_result)
            model_times['Logistic Regression'] = time.time() - model_start
            completed_models += 1
            print(f"      ‚úÖ AUC: {lr_result['auc']:.4f}, PR-AUC: {lr_result['pr_auc']:.4f}, Spearman: {lr_result['spearman']:.4f} ({model_times['Logistic Regression']/60:.1f} –º–∏–Ω)")
            print_progress()
    except Exception as e:
        print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è
    total_elapsed = time.time() - total_start_time
    hours = int(total_elapsed // 3600)
    minutes = int((total_elapsed % 3600) // 60)
    seconds = int(total_elapsed % 60)
    print(f"\n   ‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {hours:02d}:{minutes:02d}:{seconds:02d} ({total_elapsed/60:.1f} –º–∏–Ω—É—Ç)")
    print(f"   üìä –û–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(models)}")
    
    if not models:
        raise ValueError("‚ùå –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞!")
    
    # 5. –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    print("\n[5/7] –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è...")
    ensemble_result = create_ensemble(models, X_val, y_val)
    if ensemble_result:
        models.append(ensemble_result)
        print(f"      ‚úÖ –ê–Ω—Å–∞–º–±–ª—å: AUC: {ensemble_result['auc']:.4f}, PR-AUC: {ensemble_result['pr_auc']:.4f}, Spearman: {ensemble_result['spearman']:.4f}")
    
    # 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    print("\n[6/7] –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
    print("\n" + "=" * 80)
    print(f"{'–ú–æ–¥–µ–ª—å':<40} {'AUC':<10} {'PR-AUC':<10} {'Spearman':<10}")
    print("=" * 80)
    
    for model_result in models:
        spearman_val = model_result.get('spearman')
        spearman_str = f"{spearman_val:.4f}" if spearman_val is not None else "-"
        print(f"{model_result['name']:<40} {model_result['auc']:<10.4f} {model_result['pr_auc']:<10.4f} {spearman_str:<10}")
    
    # –õ—É—á—à–∞—è –ø–æ PR-AUC
    best_model = max(models, key=lambda x: x['pr_auc'])
    print("\n" + "=" * 80)
    print(f"üèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model['name']}")
    print(f"   AUC: {best_model['auc']:.4f}")
    print(f"   PR-AUC: {best_model['pr_auc']:.4f}")
    print(f"   Spearman: {best_model['spearman']:.4f}")
    
    if best_model['pr_auc'] >= 0.80:
        print("   üéâ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê! PR-AUC >= 0.80")
    elif best_model['pr_auc'] >= 0.60:
        print("   ‚úÖ –û—á–µ–Ω—å —Ö–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! PR-AUC >= 0.60")
    elif best_model['pr_auc'] >= 0.40:
        print("   ‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! PR-AUC >= 0.40")
    else:
        print("   ‚ö† –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –¥–∞–ª—å—à–µ")
    print("=" * 80)
    
    # 7. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–í–´–°–û–ö–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢)
    print("\n[7/8] –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ä–æ–≥–∞
    if isinstance(best_model['model'], list):
        # –ê–Ω—Å–∞–º–±–ª—å
        val_predictions = []
        for model_result in best_model['model']:
            model = model_result['model']
            scaler = model_result.get('scaler')
            if scaler:
                X_val_scaled = scaler.transform(X_val)
                pred = model.predict_proba(X_val_scaled)[:, 1]
            else:
                pred = model.predict_proba(X_val)[:, 1]
            val_predictions.append(pred)
        val_pred_proba = np.average(val_predictions, axis=0, weights=best_model['weights'])
    else:
        model = best_model['model']
        scaler = best_model.get('scaler')
        if scaler:
            X_val_scaled = scaler.transform(X_val)
            val_pred_proba = model.predict_proba(X_val_scaled)[:, 1]
        else:
            val_pred_proba = model.predict_proba(X_val)[:, 1]
    
    # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ F1 (–±–∞–ª–∞–Ω—Å precision –∏ recall)
    from sklearn.metrics import f1_score, precision_recall_curve
    precision, recall, thresholds = precision_recall_curve(y_val, val_pred_proba)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
    
    print(f"      ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {optimal_threshold:.4f} (–≤–º–µ—Å—Ç–æ 0.5)")
    print(f"      Precision –ø—Ä–∏ —ç—Ç–æ–º –ø–æ—Ä–æ–≥–µ: {precision[optimal_idx]:.4f}")
    print(f"      Recall –ø—Ä–∏ —ç—Ç–æ–º –ø–æ—Ä–æ–≥–µ: {recall[optimal_idx]:.4f}")
    print(f"      F1 –ø—Ä–∏ —ç—Ç–æ–º –ø–æ—Ä–æ–≥–µ: {f1_scores[optimal_idx]:.4f}")
    
    # 8. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    print("\n[8/8] –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    if isinstance(best_model['model'], list):
        # –ê–Ω—Å–∞–º–±–ª—å
        predictions = []
        for i, model_result in enumerate(best_model['model']):
            model = model_result['model']
            scaler = model_result.get('scaler')
            if scaler:
                X_test_scaled = scaler.transform(X_test)
                pred = model.predict_proba(X_test_scaled)[:, 1]
            else:
                pred = model.predict_proba(X_test)[:, 1]
            predictions.append(pred)
        
        test_pred_proba = np.average(predictions, axis=0, weights=best_model['weights'])
    else:
        # –û–¥–Ω–∞ –º–æ–¥–µ–ª—å
        model = best_model['model']
        scaler = best_model.get('scaler')
        if scaler:
            X_test_scaled = scaler.transform(X_test)
            test_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        else:
            test_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
    test_pred = (test_pred_proba >= optimal_threshold).astype(int)
    
    test_auc = roc_auc_score(y_test, test_pred_proba)
    test_pr_auc = average_precision_score(y_test, test_pred_proba)
    test_spearman, _ = spearmanr(y_test, test_pred_proba)
    test_f1 = f1_score(y_test, test_pred)
    
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞:")
    print(f"   AUC: {test_auc:.4f}")
    print(f"   PR-AUC: {test_pr_auc:.4f}")
    print(f"   Spearman: {test_spearman:.4f}")
    print(f"   F1 (–ø–æ—Ä–æ–≥ {optimal_threshold:.4f}): {test_f1:.4f}")
    
    # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print("\n[9/9] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    model_path = Path("models")
    model_path.mkdir(exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    with open(model_path / "best_model_advanced.pkl", "wb") as f:
        pickle.dump({
            "model": best_model["model"],
            "scaler": best_model.get("scaler"),
            "smote": best_model.get("smote"),
            "feature_cols": feature_cols,
            "model_name": best_model["name"],
            "auc": test_auc,
            "pr_auc": test_pr_auc,
            "spearman": test_spearman,
            "f1": test_f1,
            "best_params": best_model.get("best_params"),
            "weights": best_model.get("weights"),
            "optimal_threshold": optimal_threshold
        }, f)
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/best_model_advanced.pkl")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_df = pd.DataFrame([
        {
            "model": m["name"],
            "auc": m["auc"],
            "pr_auc": m["pr_auc"]
        }
        for m in models
    ])
    results_df.to_csv("models/model_comparison_advanced.csv", index=False)
    print(f"   ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: models/model_comparison_advanced.csv")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions_df = pd.DataFrame({
        "y_true": y_test.values,
        "y_pred": test_pred,
        "y_pred_proba": test_pred_proba
    })
    predictions_df.to_csv("models/predictions_advanced.csv", index=False)
    print(f"   ‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: models/predictions_advanced.csv")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_pipeline_time = time.time() - pipeline_start_time
    hours = int(total_pipeline_time // 3600)
    minutes = int((total_pipeline_time % 3600) // 60)
    seconds = int(total_pipeline_time % 60)
    
    print("\n" + "=" * 80)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 80)
    print(f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è pipeline: {hours:02d}:{minutes:02d}:{seconds:02d}")
    print(f"üìÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    return best_model, models, X_test, y_test, feature_cols


if __name__ == "__main__":
    best_model, all_models, X_test, y_test, feature_cols = main()
def tune_catboost_with_optuna(X_train: pd.DataFrame, y_train: pd.Series):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è CatBoost —á–µ—Ä–µ–∑ Optuna –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ PR-AUC."""
    if not OPTUNA_AVAILABLE or not CATBOOST_AVAILABLE:
        return None, None
    print("      üéØ Optuna: –Ω–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ CatBoost...")
    def objective(trial):
        params = {
            "depth": trial.suggest_int("depth", 5, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.02, 0.2, log=True),
            "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 10),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.5, 1.0),
            "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 10),
            "scale_pos_weight": trial.suggest_float("scale_pos_weight", 1.0, 5.0),
            "iterations": trial.suggest_int("iterations", 300, 800)
        }
        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)
        scores = []
        for train_idx, val_idx in cv.split(X_train, y_train):
            model = CatBoostClassifier(
                eval_metric="PRAUC",
                loss_function="Logloss",
                random_state=RANDOM_STATE,
                verbose=False,
                thread_count=CATBOOST_THREADS,
                **params
            )
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
            model.fit(X_tr, y_tr, eval_set=(X_val, y_val), use_best_model=True, verbose=False)
            y_pred = model.predict_proba(X_val)[:, 1]
            scores.append(average_precision_score(y_val, y_pred))
        return float(np.mean(scores))
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=20, show_progress_bar=False)
    best_params = study.best_params
    print(f"      ‚úÖ Optuna –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}")
    best_model = CatBoostClassifier(
        eval_metric="PRAUC",
        loss_function="Logloss",
        random_state=RANDOM_STATE,
        thread_count=CATBOOST_THREADS,
        **best_params
    )
    best_model.fit(X_train, y_train, verbose=100)
    return best_model, study
