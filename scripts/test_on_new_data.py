"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
1. –ü–æ–ª–æ–∂–∏—Ç–µ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø–∞–ø–∫—É test_data/
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python test_on_new_data.py
3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—Å—è –≤ test_data/predictions.csv

–¢–†–ï–ë–£–ï–ú–´–ï –§–ê–ô–õ–´ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º):
- test_data/application_metadata.csv
- test_data/credit_hystory.csv (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- test_data/demographics.csv (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- test_data/financial_ratios.jsonl (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- test_data/geographic_data.xml (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

–ò–õ–ò —É–∂–µ –≥–æ—Ç–æ–≤—ã–π:
- test_data/master_dataset.csv
"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    precision_recall_curve,
    f1_score,
    fbeta_score,
    matthews_corrcoef,
    classification_report,
    confusion_matrix
)

import prepare_data as prep
from train_ultimate import create_ultimate_features, handle_missing_and_encode

def load_test_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    test_path = Path("test_data")
    
    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ì–æ—Ç–æ–≤—ã–π master_dataset
    master_file = test_path / "master_dataset.csv"
    if master_file.exists():
        print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –≥–æ—Ç–æ–≤—ã–π master_dataset.csv")
        return pd.read_csv(master_file)
    
    # –í–∞—Ä–∏–∞–Ω—Ç 2: –°–æ–±–∏—Ä–∞–µ–º –∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤  
    print("üîß –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ –º–µ–Ω—è–µ–º BASE_PATH –≤ prepare_data
    original_base_path = prep.BASE_PATH
    try:
        prep.BASE_PATH = test_path
        master_df = prep.build_master_dataset()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        results_file = test_path / "results.csv"
        if results_file.exists():
            print("‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª results.csv —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
            results_df = pd.read_csv(results_file)
            
            # –î–∂–æ–π–Ω–∏–º –ø–æ customer_id
            if 'customer_id' in results_df.columns and 'customer_id' in master_df.columns:
                master_df = master_df.merge(
                    results_df[['customer_id', 'default']], 
                    on='customer_id',
                    how='left'
                )
                print(f"   ‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ —Å results.csv –ø–æ customer_id")
        
        return master_df
    finally:
        prep.BASE_PATH = original_base_path


def load_best_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
    model_files = [
        "models/best_model_optimized.pkl",  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –º–æ–¥–µ–ª—å –∏–∑ train_max_auc.py
        "models/best_model_normalized.pkl",
        "models/best_model_final.pkl",
        "models/best_model_ultimate.pkl",
        "models/best_model_advanced.pkl"
    ]
    
    for model_file in model_files:
        if Path(model_file).exists():
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_file}")
            with open(model_file, "rb") as f:
                return pickle.load(f), model_file
    
    raise FileNotFoundError(
        "–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å:\n"
        "  make train\n"
        "  –∏–ª–∏\n"
        "  python scripts/train_max_auc.py"
    )


def predict_on_new_data(test_df, model_data, selected_features=None):
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º."""
    
    # Feature engineering (—Ç–µ –∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
    print("üîß Feature Engineering...")
    test_featured = create_ultimate_features(test_df)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ (–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ, –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
    scaler = model_data.get('scaler')
    X_test, y_test, feature_cols, _ = handle_missing_and_encode(test_featured, scaler=None)  # –°–Ω–∞—á–∞–ª–∞ –±–µ–∑ scaler
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    train_features = model_data.get('feature_cols', model_data.get('selected_features'))
    if train_features is not None:
        print(f"üîß –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(X_test.columns)} ‚Üí {len(train_features)}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª—è–º–∏
        missing_features = set(train_features) - set(X_test.columns)
        if missing_features:
            print(f"   ‚ö† –î–æ–±–∞–≤–ª—è–µ–º {len(missing_features)} –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            for feat in missing_features:
                X_test[feat] = 0
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        extra_features = set(X_test.columns) - set(train_features)
        if extra_features:
            print(f"   ‚ö† –£–¥–∞–ª—è–µ–º {len(extra_features)} –ª–∏—à–Ω–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            X_test = X_test.drop(columns=list(extra_features))
        
        # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ
        X_test = X_test[train_features]
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
    if scaler is not None:
        print(f"   üîß –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (StandardScaler)")
        X_test_scaled = scaler.transform(X_test)
        X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    models = model_data.get('models')
    meta_model = model_data.get('meta_model')
    strategy = model_data.get('strategy', 'averaging')
    optimal_weights = model_data.get('optimal_weights')  # –î–ª—è —Å—Ç–∞—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π
    optimal_threshold = model_data.get('optimal_threshold', 0.5)
    
    if models is None:
        raise ValueError("–í –º–æ–¥–µ–ª–∏ –Ω–µ—Ç –∞–Ω—Å–∞–º–±–ª—è!")
    
    print(f"üéØ –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–Ω—Å–∞–º–±–ª—å –∏–∑ {len(models)} –º–æ–¥–µ–ª–µ–π...")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    predictions = []
    for i, model in enumerate(models):
        if hasattr(model, 'predict_proba'):
            pred = model.predict_proba(X_test)[:, 1]
        else:
            pred = model.predict(X_test)
        predictions.append(pred)
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å {i+1}/{len(models)}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é (—Å—Ç–µ–∫–∏–Ω–≥ –∏–ª–∏ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ)
    if strategy == 'stacking' and meta_model is not None:
        # –°—Ç–µ–∫–∏–Ω–≥: –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å
        meta_features = np.array(predictions).T
        y_pred_proba = meta_model.predict_proba(meta_features)[:, 1]
        print(f"   ‚úÖ –°—Ç–µ–∫–∏–Ω–≥ (meta-model)")
    elif optimal_weights is not None:
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ (–¥–ª—è —Å—Ç–∞—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π)
        y_pred_proba = np.average(predictions, axis=0, weights=optimal_weights)
        print(f"   ‚úÖ –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ: {optimal_weights}")
    else:
        # –ü—Ä–æ—Å—Ç–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ
        y_pred_proba = np.mean(predictions, axis=0)
        print(f"   ‚úÖ –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ ({strategy})")
    
    # –ë–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = (y_pred_proba >= optimal_threshold).astype(int)
    print(f"   ‚úÖ –ü–æ—Ä–æ–≥: {optimal_threshold:.4f}")
    
    return y_pred, y_pred_proba, y_test


def evaluate_predictions(y_true, y_pred, y_pred_proba, has_labels=True):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π."""
    
    if not has_labels:
        print("\n" + "="*80)
        print("‚ö† –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'default' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        print("="*80)
        
        print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        print(pd.Series(y_pred).value_counts(normalize=True))
        print(f"\n–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–µ—Ñ–æ–ª—Ç–∞: {y_pred_proba.mean():.4f}")
        print(f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {np.median(y_pred_proba):.4f}")
        return
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –µ—Å—Ç—å –º–µ—Ç–∫–∞
    valid_mask = ~y_true.isna()
    if valid_mask.sum() == 0:
        print("\n‚ö† –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ - —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        has_labels = False
        evaluate_predictions(y_true, y_pred, y_pred_proba, has_labels=False)
        return
    
    if valid_mask.sum() < len(y_true):
        print(f"\n‚ö† –¢–æ–ª—å–∫–æ {valid_mask.sum()} –∏–∑ {len(y_true)} –∏–º–µ—é—Ç –º–µ—Ç–∫–∏ - –æ—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∏—Ö")
        y_true = y_true[valid_mask]
        y_pred = y_pred[valid_mask]
        y_pred_proba = y_pred_proba[valid_mask]
    
    print("\n" + "="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –ù–û–í–´–• –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•")
    print("="*80)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    auc = roc_auc_score(y_true, y_pred_proba)
    pr_auc = average_precision_score(y_true, y_pred_proba)
    f1 = f1_score(y_true, y_pred)
    f2 = fbeta_score(y_true, y_pred, beta=2)
    mcc = matthews_corrcoef(y_true, y_pred)
    
    print(f"\n{'–ú–µ—Ç—Ä–∏–∫–∞':<25} {'–ó–Ω–∞—á–µ–Ω–∏–µ':<10}")
    print("="*35)
    print(f"{'AUC-ROC':<25} {auc:<10.4f}")
    print(f"{'PR-AUC (–ì–õ–ê–í–ù–ê–Ø)':<25} {pr_auc:<10.4f}")
    print(f"{'F1-Score':<25} {f1:<10.4f}")
    print(f"{'F2-Score':<25} {f2:<10.4f}")
    print(f"{'Matthews Correlation':<25} {mcc:<10.4f}")
    print("="*35)
    
    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    print(f"\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
    print(f"  TN: {cm[0,0]:>6}  |  FP: {cm[0,1]:>6}")
    print(f"  FN: {cm[1,0]:>6}  |  TP: {cm[1,1]:>6}")
    
    # Classification Report
    print(f"\n–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
    print(classification_report(y_true, y_pred, target_names=['No Default', 'Default']))
    
    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)
    f2_scores = (5 * precision * recall) / (4 * precision + recall + 1e-6)
    optimal_idx = np.argmax(f2_scores)
    optimal_new_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
    
    print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {optimal_new_threshold:.4f}")
    print(f"   (Precision: {precision[optimal_idx]:.4f}, Recall: {recall[optimal_idx]:.4f})")


def main():
    print("="*80)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ù–ê –ù–û–í–´–• –î–ê–ù–ù–´–•")
    print("="*80)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\n[1/4] –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    try:
        test_df = load_test_data()
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {test_df.shape[0]} —Å—Ç—Ä–æ–∫, {test_df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        has_labels = 'default' in test_df.columns
        if has_labels:
            print(f"   ‚úÖ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–∞–π–¥–µ–Ω–∞. –î–æ–ª—è –¥–µ—Ñ–æ–ª—Ç–æ–≤: {test_df['default'].mean():.2%}")
        else:
            print(f"   ‚ö† –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'default' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç - —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
            
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("\n[2/4] –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    try:
        model_data, model_file = load_best_model()
        selected_features = model_data.get('selected_features')
        print(f"   ‚úÖ PR-AUC –º–æ–¥–µ–ª–∏: {model_data.get('pr_auc', 'N/A')}")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return
    
    # 3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("\n[3/4] –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    try:
        y_pred, y_pred_proba, y_test = predict_on_new_data(
            test_df, model_data, selected_features
        )
        print(f"   ‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã –¥–ª—è {len(y_pred)} –æ–±—ä–µ–∫—Ç–æ–≤")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 4. –û—Ü–µ–Ω–∫–∞
    print("\n[4/4] –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    evaluate_predictions(y_test, y_pred, y_pred_proba, has_labels=has_labels)
    
    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print("\n[–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...]")
    output_df = pd.DataFrame({
        'prediction': y_pred,
        'probability': y_pred_proba
    })
    
    if has_labels:
        output_df['actual'] = y_test.values
    
    # –î–æ–±–∞–≤–ª—è–µ–º ID –µ—Å–ª–∏ –µ—Å—Ç—å
    if 'customer_id' in test_df.columns:
        output_df['customer_id'] = test_df['customer_id'].values
    elif 'application_id' in test_df.columns:
        output_df['application_id'] = test_df['application_id'].values
    
    output_path = Path("test_data") / "predictions.csv"
    output_df.to_csv(output_path, index=False)
    print(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
    
    print("\n" + "="*80)
    print("‚úÖ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("="*80)


if __name__ == "__main__":
    main()
