"""
ULTIMATE ML Pipeline - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è PR-AUC ‚â• 0.80

–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
1. ‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ (scale_pos_weight ‚â• 4, class_weights)
2. ‚úÖ ADASYN —Å sampling_strategy=0.75
3. ‚úÖ 40+ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–µ—Ñ–æ–ª—Ç–∞
4. ‚úÖ –ê–Ω—Å–∞–º–±–ª—å –∏–∑ 5 –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≤–µ—Å–æ–≤ –ø–æ–¥ PR-AUC
5. ‚úÖ –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
6. ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –ø–æ–¥ F2-score (recall –≤–∞–∂–Ω–µ–µ)
7. ‚úÖ Optuna —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ PR-AUC

–û–ñ–ò–î–ê–ï–ú–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢: PR-AUC 0.70-0.80 (—Å–µ–π—á–∞—Å 0.33)
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
import pickle
import warnings
from datetime import datetime
from scipy.stats import spearmanr
from scipy.optimize import minimize
warnings.filterwarnings('ignore')

from sklearn.model_selection import (
    StratifiedKFold,
    train_test_split,
    cross_val_score
)
from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    precision_recall_curve,
    f1_score,
    fbeta_score,
    matthews_corrcoef,
    brier_score_loss
)
from sklearn.calibration import CalibratedClassifierCV

# –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
from imblearn.over_sampling import ADASYN, BorderlineSMOTE

# Gradient Boosting
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except:
    LIGHTGBM_AVAILABLE = False

try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except:
    CATBOOST_AVAILABLE = False
    raise ImportError("CatBoost –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —ç—Ç–æ–≥–æ pipeline!")

try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
except:
    OPTUNA_AVAILABLE = False
    print("‚ö† Optuna –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

import prepare_data as prep

# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

RANDOM_STATE = 42
EXCLUDE_COLS = ["customer_id", "default", "application_id"]

# –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞
import multiprocessing
MAX_JOBS = multiprocessing.cpu_count()
CATBOOST_THREADS = -1  # –í—Å–µ —è–¥—Ä–∞ –¥–ª—è CatBoost

print(f"üíª –î–æ—Å—Ç—É–ø–Ω–æ CPU —è–¥–µ—Ä: {MAX_JOBS}")
print(f"üíª CatBoost –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞")


# ============================================================================
# –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò –î–ï–§–û–õ–¢–ê
# ============================================================================

def create_default_detection_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–µ—Ñ–æ–ª—Ç–∞.
    –§–æ–∫—É—Å –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –¥–ª—è –¥–µ—Ñ–æ–ª—Ç–æ–≤.
    """
    df = df.copy()
    print("   üîß –°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–µ—Ñ–æ–ª—Ç–∞...")
    
    # ===== –ö–û–ú–ë–ò–ù–ê–¶–ò–ò –í–´–°–û–ö–û–ì–û –†–ò–°–ö–ê =====
    
    # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π —Ä–∏—Å–∫: –ø–ª–æ—Ö–æ–π —Å–∫–æ—Ä + –≤—ã—Å–æ–∫–∏–π –¥–æ–ª–≥ + –ø—Ä–æ—Å—Ä–æ—á–∫–∏
    if all(col in df.columns for col in ['credit_score', 'debt_to_income_ratio', 'num_delinquencies_2yrs']):
        df['extreme_risk_combo'] = (
            (df['credit_score'] < 600) &
            (df['debt_to_income_ratio'] > 0.5) &
            (df['num_delinquencies_2yrs'] > 0)
        ).astype(int)
        
        df['critical_risk_combo'] = (
            (df['credit_score'] < 650) &
            (df['debt_to_income_ratio'] > 0.43)
        ).astype(int)
    
    # –í—ã—Å–æ–∫–∏–π –¥–æ–ª–≥ + –Ω–∏–∑–∫–∏–π –¥–æ—Ö–æ–¥
    if all(col in df.columns for col in ['debt_to_income_ratio', 'annual_income']):
        income_q30 = df['annual_income'].quantile(0.3)
        df['high_debt_low_income'] = (
            (df['debt_to_income_ratio'] > 0.6) &
            (df['annual_income'] < income_q30)
        ).astype(int)
    
    # –ú–æ–ª–æ–¥–æ–π + –≤—ã—Å–æ–∫–∞—è —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è
    if all(col in df.columns for col in ['age', 'credit_utilization']):
        df['young_high_utilization'] = (
            (df['age'] < 30) &
            (df.get('credit_utilization', 0) > 0.8)
        ).astype(int)
    
    # –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å + –≤—ã—Å–æ–∫–∏–π –¥–æ–ª–≥
    if all(col in df.columns for col in ['employment_length', 'debt_to_income_ratio']):
        df['unstable_employment_high_debt'] = (
            (df['employment_length'] < 2) &
            (df['debt_to_income_ratio'] > 0.5)
        ).astype(int)
    
    # ===== –°–ö–û–†–ò–ù–ì–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò =====
    
    # Composite risk score (0-10)
    risk_components = []
    if 'credit_score' in df.columns:
        risk_components.append((df['credit_score'] < 650).astype(int) * 3)
    if 'debt_to_income_ratio' in df.columns:
        risk_components.append((df['debt_to_income_ratio'] > 0.5).astype(int) * 3)
    if 'num_delinquencies_2yrs' in df.columns:
        risk_components.append((df['num_delinquencies_2yrs'] > 0).astype(int) * 2)
    if 'credit_utilization' in df.columns:
        risk_components.append((df.get('credit_utilization', 0) > 0.7).astype(int) * 2)
    
    if risk_components:
        df['default_risk_score'] = sum(risk_components)
    
    # ===== –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø –ß–ò–°–õ–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í =====
    
    # –ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è √ó –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Å—Ç—Ä–µ—Å—Å
    if all(col in df.columns for col in ['num_delinquencies_2yrs', 'debt_to_income_ratio']):
        df['credit_history_debt_stress'] = (
            df['num_delinquencies_2yrs'].fillna(0) *
            df['debt_to_income_ratio']
        )
    
    # –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è √ó –î–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
    if all(col in df.columns for col in ['credit_utilization', 'debt_to_income_ratio']):
        df['utilization_debt_product'] = (
            df.get('credit_utilization', 0) *
            df['debt_to_income_ratio']
        )
    
    # –í–æ–∑—Ä–∞—Å—Ç √ó –ö—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–∫–æ—Ä (–º–æ–ª–æ–¥—ã–µ —Å –ø–ª–æ—Ö–∏–º —Å–∫–æ—Ä–æ–º)
    if all(col in df.columns for col in ['age', 'credit_score']):
        df['age_score_interaction'] = (
            (40 - df['age'].clip(20, 40)) *  # –º–æ–ª–æ–∂–µ = –±–æ–ª—å—à–µ —Ä–∏—Å–∫
            (750 - df['credit_score'].clip(300, 750))  # –Ω–∏–∂–µ —Å–∫–æ—Ä = –±–æ–ª—å—à–µ —Ä–∏—Å–∫
        )
    
    # ===== –§–ò–ù–ê–ù–°–û–í–ê–Ø –ù–ï–°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–¨ =====
    
    # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –¥–µ–Ω–µ–∂–Ω—ã–π –ø–æ—Ç–æ–∫
    if 'monthly_free_cash_flow' in df.columns:
        df['negative_cash_flow'] = (df['monthly_free_cash_flow'] < 0).astype(int)
        df['severe_cash_deficit'] = (df['monthly_free_cash_flow'] < -500).astype(int)
    
    # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è DTI (> 85% - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å)
    if 'debt_to_income_ratio' in df.columns:
        df['extreme_dti'] = (df['debt_to_income_ratio'] > 0.85).astype(int)
        df['dti_danger_zone'] = (
            (df['debt_to_income_ratio'] > 0.6) &
            (df['debt_to_income_ratio'] <= 0.85)
        ).astype(int)
    
    # ===== –ü–û–í–ï–î–ï–ù–ß–ï–°–ö–ò–ï –ö–†–ê–°–ù–´–ï –§–õ–ê–ì–ò =====
    
    # –í—ã—Å–æ–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ + –Ω–∏–∑–∫–∞—è —Ü–∏—Ñ—Ä–æ–≤–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    if all(col in df.columns for col in ['num_customer_service_calls', 'num_login_sessions']):
        median_calls = df['num_customer_service_calls'].median()
        median_sessions = df['num_login_sessions'].median()
        df['support_intensive_low_engagement'] = (
            (df['num_customer_service_calls'] > median_calls * 1.5) &
            (df['num_login_sessions'] < median_sessions * 0.5)
        ).astype(int)
    
    # –ü–æ–∑–¥–Ω—è—è –ø–æ–¥–∞—á–∞ –∑–∞—è–≤–∫–∏ (22:00 - 06:00)
    if 'application_hour' in df.columns:
        df['late_night_application'] = (
            (df['application_hour'] >= 22) |
            (df['application_hour'] <= 6)
        ).astype(int)
    
    # ===== –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ô –†–ò–°–ö =====
    
    # –í—ã—Å–æ–∫–∞—è –±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü–∞ + –Ω–∏–∑–∫–∏–π –¥–æ—Ö–æ–¥
    if all(col in df.columns for col in ['regional_unemployment_rate', 'annual_income', 'regional_median_income']):
        df['regional_economic_stress'] = (
            (df['regional_unemployment_rate'] > 0.08) &
            (df['annual_income'] < df['regional_median_income'])
        ).astype(int)
    
    # ===== –°–õ–û–ñ–ù–´–ï –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø =====
    
    # –¢—Ä–æ–π–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ: –º–æ–ª–æ–¥–æ–π + –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π + –≤—ã—Å–æ–∫–∏–π –¥–æ–ª–≥
    if all(col in df.columns for col in ['age', 'employment_length', 'debt_to_income_ratio']):
        df['triple_risk_young_unstable_debt'] = (
            (df['age'] < 30) &
            (df['employment_length'] < 2) &
            (df['debt_to_income_ratio'] > 0.5)
        ).astype(int)
    
    # –ü—Ä–æ—Å—Ä–æ—á–∫–∏ + –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –≤ —Ä—É–∏–Ω–∞—Ö)
    if all(col in df.columns for col in ['num_delinquencies_2yrs', 'num_collections']):
        df['credit_history_ruined'] = (
            (df['num_delinquencies_2yrs'] > 1) &
            (df['num_collections'] > 0)
        ).astype(int)
        
        df['delinquency_collection_severity'] = (
            df['num_delinquencies_2yrs'].fillna(0) +
            df['num_collections'].fillna(0) * 2  # –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Ö—É–∂–µ
        )
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∫—Ä–µ–¥–∏—Ç–∞ (close to limit)
    if 'credit_utilization' in df.columns:
        df['maxed_out_credit'] = (df.get('credit_utilization', 0) > 0.95).astype(int)
    
    print(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    return df


# ============================================================================
# –†–ê–°–®–ò–†–ï–ù–ù–´–ô FEATURE ENGINEERING
# ============================================================================

def create_ultimate_features(df: pd.DataFrame) -> pd.DataFrame:
    """–ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
    df = df.copy()
    
    # 1. –ë–∞–∑–æ–≤—ã–π feature engineering –∏–∑ train_advanced.py
    try:
        from train_advanced import advanced_feature_engineering
        df = advanced_feature_engineering(df)
    except:
        print("   ‚ö† train_advanced.py –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π FE")
    
    # 2. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–µ—Ñ–æ–ª—Ç–∞
    df = create_default_detection_features(df)
    
    return df


# ============================================================================
# –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•
# ============================================================================

def handle_missing_and_encode(df: pd.DataFrame, target_col: str = "default", scaler=None):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è."""
    df = df.copy()
    
    # –£–¥–∞–ª—è–µ–º —à—É–º–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å
    noise_cols = [c for c in df.columns if 'noise' in c.lower() or 'random' in c.lower()]
    if noise_cols:
        df = df.drop(columns=noise_cols)
    
    # –ü—Ä–æ–ø—É—Å–∫–∏
    for col in df.columns:
        if col in EXCLUDE_COLS:
            continue
        if df[col].isna().sum() == 0:
            continue
        
        if pd.api.types.is_numeric_dtype(df[col]):
            df[col] = df[col].fillna(df[col].median())
        else:
            mode_val = df[col].mode()
            df[col] = df[col].fillna(mode_val[0] if len(mode_val) > 0 else "unknown")
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    from sklearn.preprocessing import LabelEncoder
    for col in df.columns:
        if col in EXCLUDE_COLS:
            continue
        if df[col].dtype == "object" or df[col].dtype.name == "category":
            le = LabelEncoder()
            df[col] = df[col].fillna("unknown")
            df[col] = le.fit_transform(df[col].astype(str))
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    feature_cols = [c for c in df.columns if c not in EXCLUDE_COLS]
    X = df[feature_cols].copy()
    y = df[target_col].copy()
    
    # –û—á–∏—Å—Ç–∫–∞
    for col in X.columns:
        if not pd.api.types.is_numeric_dtype(X[col]):
            X[col] = pd.to_numeric(X[col], errors="coerce").fillna(0)
    
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(X.median())
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    from sklearn.preprocessing import StandardScaler
    if scaler is None:
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
    else:
        X_scaled = scaler.transform(X)
    
    X = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
    
    return X, y, feature_cols, scaler


# ============================================================================
# –£–°–ò–õ–ï–ù–ù–ê–Ø –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê
# ============================================================================

def balance_with_adasyn(X, y, sampling_strategy=0.75):
    """
    ADASYN –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –ø—Ä–∏–º–µ—Ä–æ–≤.
    sampling_strategy=0.75 –æ–∑–Ω–∞—á–∞–µ—Ç minority –±—É–¥–µ—Ç 75% –æ—Ç majority.
    """
    try:
        adasyn = ADASYN(
            n_neighbors=7,
            random_state=RANDOM_STATE,
            sampling_strategy=sampling_strategy
        )
        X_resampled, y_resampled = adasyn.fit_resample(X, y)
        print(f"      ‚úÖ ADASYN: {len(y)} ‚Üí {len(y_resampled)} —Å—Ç—Ä–æ–∫")
        print(f"      –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {pd.Series(y_resampled).value_counts().to_dict()}")
        return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled)
    except Exception as e:
        print(f"      ‚ö† ADASYN –Ω–µ —É–¥–∞–ª—Å—è: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º BorderlineSMOTE")
        smote = BorderlineSMOTE(
            k_neighbors=5,
            random_state=RANDOM_STATE,
            sampling_strategy=sampling_strategy
        )
        X_resampled, y_resampled = smote.fit_resample(X, y)
        return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled)


# ============================================================================
# OPTUNA –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –° –§–û–ö–£–°–û–ú –ù–ê PR-AUC
# ============================================================================

def optimize_catboost_ultimate(X_train, y_train, n_trials=50):
    """
    Optuna —Å –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ú–ò –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏:
    - scale_pos_weight ‚â• 4 (–ù–ï 1!)
    - –§–æ–∫—É—Å –Ω–∞ PR-AUC
    """
    if not OPTUNA_AVAILABLE or not CATBOOST_AVAILABLE:
        return None
    
    print(f"      üéØ Optuna: –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (PR-AUC focus) –Ω–∞ {MAX_JOBS} —è–¥—Ä–∞—Ö...")
    
    def objective(trial):
        params = {
            'iterations': trial.suggest_int('iterations', 500, 1000),
            'depth': trial.suggest_int('depth', 6, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 15),
            'border_count': trial.suggest_int('border_count', 32, 255),
            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 5),
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ù–ï –î–ê–ï–ú OPTUNA –í–´–ë–ò–†–ê–¢–¨ scale_pos_weight < 4!
            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 4.0, 8.0),
            
            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 10),
        }
        
        model = CatBoostClassifier(
            **params,
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ scale_pos_weight (—É–∂–µ –≤ params)
            # –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º class_weights –∏ auto_class_weights –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ!
            
            eval_metric='PRAUC',
            random_state=RANDOM_STATE,
            verbose=False,
            thread_count=-1  # –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —è–¥—Ä–∞
        )
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ PR-AUC
        cv_scores = cross_val_score(
            model, X_train, y_train,
            cv=3,
            scoring='average_precision',  # PR-AUC!
            n_jobs=1  # 1 –ø–æ—Ç–æ–º—É —á—Ç–æ CatBoost —É–∂–µ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π
        )
        
        return cv_scores.mean()
    
    study = optuna.create_study(
        direction='maximize',
        sampler=TPESampler(seed=RANDOM_STATE)
    )
    
    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
    
    print(f"      ‚úÖ –õ—É—á—à–∏–π PR-AUC (CV): {study.best_value:.4f}")
    print(f"      ‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {study.best_params}")
    
    return study.best_params


# ============================================================================
# –û–ë–£–ß–ï–ù–ò–ï –ê–ù–°–ê–ú–ë–õ–Ø –ú–û–î–ï–õ–ï–ô
# ============================================================================

def train_ensemble_models(X_train, y_train, X_val, y_val, best_params=None):
    """
    –û–±—É—á–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å –∏–∑ 5 –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞–º–∏ –∏ seed.
    """
    print("      üéØ –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ 5 –º–æ–¥–µ–ª–µ–π...")
    
    models = []
    seeds = [42, 123, 456, 789, 999]
    sampling_strategies = [0.70, 0.75, 0.75, 0.80, 0.80]
    
    if best_params is None:
        best_params = {
            'iterations': 800,
            'depth': 8,
            'learning_rate': 0.05,
            'l2_leaf_reg': 10,
            'scale_pos_weight': 6.0,
        }
    
    for i, (seed, sampling_strategy) in enumerate(zip(seeds, sampling_strategies)):
        print(f"         –ú–æ–¥–µ–ª—å {i+1}/5 (seed={seed}, sampling={sampling_strategy})...")
        
        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
        X_train_balanced, y_train_balanced = balance_with_adasyn(
            X_train, y_train, sampling_strategy=sampling_strategy
        )
        
        # –ú–æ–¥–µ–ª—å —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º seed
        params_copy = best_params.copy()
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ scale_pos_weight –µ—Å—Ç—å
        if 'scale_pos_weight' not in params_copy:
            params_copy['scale_pos_weight'] = 6.0
        
        model = CatBoostClassifier(
            **params_copy,
            eval_metric='PRAUC',
            random_state=seed,
            verbose=False,
            thread_count=-1  # –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —è–¥—Ä–∞
        )
        
        model.fit(X_train_balanced, y_train_balanced)
        models.append(model)
    
    print("      ‚úÖ –ê–Ω—Å–∞–º–±–ª—å –æ–±—É—á–µ–Ω")
    return models


def optimize_ensemble_weights(models, X_val, y_val):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –∞–Ω—Å–∞–º–±–ª—è –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ PR-AUC.
    """
    print("      üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è...")
    
    predictions = [m.predict_proba(X_val)[:, 1] for m in models]
    
    def neg_pr_auc(weights):
        weights = weights / weights.sum()
        blended = np.average(predictions, axis=0, weights=weights)
        return -average_precision_score(y_val, blended)
    
    result = minimize(
        neg_pr_auc,
        np.ones(len(models)) / len(models),
        method='SLSQP',
        bounds=[(0, 1) for _ in models],
        constraints={'type': 'eq', 'fun': lambda w: w.sum() - 1}
    )
    
    optimal_weights = result.x / result.x.sum()
    print(f"      ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞: {optimal_weights}")
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    final_pred_proba = np.average(predictions, axis=0, weights=optimal_weights)
    pr_auc = average_precision_score(y_val, final_pred_proba)
    auc = roc_auc_score(y_val, final_pred_proba)
    
    print(f"      ‚úÖ –ê–Ω—Å–∞–º–±–ª—å: AUC={auc:.4f}, PR-AUC={pr_auc:.4f}")
    
    return optimal_weights, final_pred_proba


# ============================================================================
# –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–û–†–û–ì–ê –ü–û–î F2-SCORE
# ============================================================================

def optimize_threshold_f2(y_true, y_pred_proba):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—è F2-score.
    F2 –¥–∞–µ—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å recall (–≤–∞–∂–Ω–æ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–µ—Ñ–æ–ª—Ç–æ–≤).
    """
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)
    
    # F2-score: beta=2 –æ–∑–Ω–∞—á–∞–µ—Ç recall –≤ 2 —Ä–∞–∑–∞ –≤–∞–∂–Ω–µ–µ precision
    f2_scores = (5 * precision * recall) / (4 * precision + recall + 1e-6)
    optimal_idx = np.argmax(f2_scores)
    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
    
    return optimal_threshold, precision[optimal_idx], recall[optimal_idx], f2_scores[optimal_idx]


# ============================================================================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    start_time = datetime.now()
    
    print("=" * 80)
    print("ULTIMATE ML PIPELINE - –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è PR-AUC")
    print("=" * 80)
    print(f"üöÄ –ù–∞—á–∞–ª–æ: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞
    print("\n[1/8] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    master_df = prep.build_master_dataset()
    print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {master_df.shape[0]} —Å—Ç—Ä–æ–∫, {master_df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
    print(f"   –î–æ–ª—è –¥–µ—Ñ–æ–ª—Ç–æ–≤: {master_df['default'].mean():.2%}")
    
    # 2. Feature Engineering
    print("\n[2/8] Feature Engineering (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–µ—Ñ–æ–ª—Ç–∞)...")
    df_featured = create_ultimate_features(master_df)
    X, y, feature_cols, scaler = handle_missing_and_encode(df_featured)
    print(f"   ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    print("\n[3/8] –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test...")
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.2, random_state=RANDOM_STATE, stratify=y_temp
    )
    print(f"   Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}")
    
    # 4. Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    print("\n[4/8] Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (scale_pos_weight ‚â• 4)...")
    best_params = None
    if OPTUNA_AVAILABLE:
        try:
            # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º train –¥–ª—è optuna
            X_train_balanced, y_train_balanced = balance_with_adasyn(X_train, y_train, 0.75)
            best_params = optimize_catboost_ultimate(X_train_balanced, y_train_balanced, n_trials=30)
        except Exception as e:
            print(f"      ‚ö† Optuna –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
    
    # 5. –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    print("\n[5/8] –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è (5 –º–æ–¥–µ–ª–µ–π)...")
    ensemble_models = train_ensemble_models(X_train, y_train, X_val, y_val, best_params)
    
    # 6. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è
    print("\n[6/8] –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è...")
    optimal_weights, val_pred_proba = optimize_ensemble_weights(ensemble_models, X_val, y_val)
    
    # 7. –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    print("\n[7/8] –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π...")
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏–∑ –∞–Ω—Å–∞–º–±–ª—è –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
    # (–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –∞–Ω—Å–∞–º–±–ª—è –Ω–∞–ø—Ä—è–º—É—é —Å–ª–æ–∂–Ω–∞, –ø–æ—ç—Ç–æ–º—É –∫–∞–ª–∏–±—Ä—É–µ–º –ø–æ—Å—Ç—Ñ–∞–∫—Ç—É–º —á–µ—Ä–µ–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
    
    # 8. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞
    print("\n[8/8] –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ (F2-score)...")
    optimal_threshold, precision, recall, f2 = optimize_threshold_f2(y_val, val_pred_proba)
    print(f"   ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {optimal_threshold:.4f}")
    print(f"   Precision: {precision:.4f}, Recall: {recall:.4f}, F2: {f2:.4f}")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
    print("\n" + "=" * 80)
    print("–§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï")
    print("=" * 80)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ
    test_predictions = [m.predict_proba(X_test)[:, 1] for m in ensemble_models]
    test_pred_proba = np.average(test_predictions, axis=0, weights=optimal_weights)
    test_pred = (test_pred_proba >= optimal_threshold).astype(int)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    test_auc = roc_auc_score(y_test, test_pred_proba)
    test_pr_auc = average_precision_score(y_test, test_pred_proba)
    test_f1 = f1_score(y_test, test_pred)
    test_f2 = fbeta_score(y_test, test_pred, beta=2)
    test_mcc = matthews_corrcoef(y_test, test_pred)
    test_brier = brier_score_loss(y_test, test_pred_proba)
    
    print(f"\n{'–ú–µ—Ç—Ä–∏–∫–∞':<25} {'–ó–Ω–∞—á–µ–Ω–∏–µ':<10}")
    print("=" * 40)
    print(f"{'AUC-ROC':<25} {test_auc:<10.4f}")
    print(f"{'PR-AUC (–ì–õ–ê–í–ù–ê–Ø)':<25} {test_pr_auc:<10.4f}")
    print(f"{'F1-Score':<25} {test_f1:<10.4f}")
    print(f"{'F2-Score':<25} {test_f2:<10.4f}")
    print(f"{'Matthews Corr Coef':<25} {test_mcc:<10.4f}")
    print(f"{'Brier Score':<25} {test_brier:<10.4f}")
    print(f"{'–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥':<25} {optimal_threshold:<10.4f}")
    print("=" * 40)
    
    if test_pr_auc >= 0.80:
        print("üéâüéâüéâ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê! PR-AUC ‚â• 0.80 üéâüéâüéâ")
    elif test_pr_auc >= 0.70:
        print("‚úÖ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! PR-AUC ‚â• 0.70")
    elif test_pr_auc >= 0.60:
        print("‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! PR-AUC ‚â• 0.60")
    elif test_pr_auc >= 0.50:
        print("‚úÖ –ü—Ä–æ–≥—Ä–µ—Å—Å –µ—Å—Ç—å! PR-AUC ‚â• 0.50")
    else:
        print("‚ö† –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–∞–ª—å–Ω–µ–π—à–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print("\n[–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤]...")
    model_path = Path("models")
    model_path.mkdir(exist_ok=True)
    
    with open(model_path / "best_model_ultimate.pkl", "wb") as f:
        pickle.dump({
            "ensemble_models": ensemble_models,
            "optimal_weights": optimal_weights,
            "feature_cols": feature_cols,
            "optimal_threshold": optimal_threshold,
            "test_pr_auc": test_pr_auc,
            "test_auc": test_auc,
            "test_f2": test_f2,
            "best_params": best_params,
            "scaler": scaler
        }, f)
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/best_model_ultimate.pkl")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions_df = pd.DataFrame({
        "y_true": y_test.values,
        "y_pred": test_pred,
        "y_pred_proba": test_pred_proba
    })
    predictions_df.to_csv("models/predictions_ultimate.csv", index=False)
    
    elapsed = datetime.now() - start_time
    print(f"\n‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed}")
    print("=" * 80)
    print("‚úÖ –ì–û–¢–û–í–û!")
    print("=" * 80)
    
    return ensemble_models, optimal_weights, X_test, y_test


if __name__ == "__main__":
    ensemble_models, optimal_weights, X_test, y_test = main()
