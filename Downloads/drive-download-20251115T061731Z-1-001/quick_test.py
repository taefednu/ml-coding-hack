#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–ª–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
"""
import pandas as pd
import numpy as np
import pickle
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

def main():
    print("=" * 80)
    print("üöÄ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•")
    print("=" * 80)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    model_path = Path('models/best_model_optimized.pkl')
    
    if not model_path.exists():
        print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏–µ.")
        return
    
    with open(model_path, 'rb') as f:
        model_dict = pickle.load(f)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
    if isinstance(model_dict, dict):
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        print(f"  –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {list(model_dict.keys())}")
        
        if 'meta_model' in model_dict:
            model = model_dict['meta_model']
            feature_cols = model_dict.get('feature_cols', None)
            selected_features = model_dict.get('selected_features', None)
            scaler = model_dict.get('scaler', None)
            optimal_threshold = model_dict.get('optimal_threshold', 0.5)
        else:
            print("‚ùå meta_model –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏!")
            return
    else:
        model = model_dict
        feature_cols = None
        selected_features = None
        scaler = None
        optimal_threshold = 0.5
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_dir = Path('test_data')
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤
    required_files = [
        'application_metadata.csv',
        'demographics.csv',
        'credit_hystory.csv',
        'financial_ratios.jsonl',
        'geographic_data.xml'
    ]
    
    missing_files = [f for f in required_files if not (test_dir / f).exists()]
    if missing_files:
        print(f"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {missing_files}")
        print("–ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        # Application metadata
        app = pd.read_csv(test_dir / 'application_metadata.csv')
        print(f"  ‚úì application_metadata: {len(app):,} –∑–∞–ø–∏—Å–µ–π")
        
        # Demographics
        demo = pd.read_csv(test_dir / 'demographics.csv')
        print(f"  ‚úì demographics: {len(demo):,} –∑–∞–ø–∏—Å–µ–π")
        
        # Credit history
        if (test_dir / 'credit_hystory.csv').exists():
            credit = pd.read_csv(test_dir / 'credit_hystory.csv')
        else:
            credit = pd.read_csv(test_dir / 'credit_history.csv')
        print(f"  ‚úì credit_history: {len(credit):,} –∑–∞–ø–∏—Å–µ–π")
        
        # Financial ratios
        financial = pd.read_json(test_dir / 'financial_ratios.jsonl', lines=True)
        print(f"  ‚úì financial_ratios: {len(financial):,} –∑–∞–ø–∏—Å–µ–π")
        
        # Geographic data
        geographic = pd.read_xml(test_dir / 'geographic_data.xml')
        print(f"  ‚úì geographic_data: {len(geographic):,} –∑–∞–ø–∏—Å–µ–π")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print("\nüîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    df = app.copy()
    
    # Rename ID columns to customer_ref
    if 'customer_ref' not in demo.columns:
        if 'cust_id' in demo.columns:
            demo = demo.rename(columns={'cust_id': 'customer_ref'})
        elif 'id' in demo.columns:
            demo = demo.rename(columns={'id': 'customer_ref'})
    
    if 'customer_ref' not in credit.columns:
        if 'customer_number' in credit.columns:
            credit = credit.rename(columns={'customer_number': 'customer_ref'})
        elif 'cust_id' in credit.columns:
            credit = credit.rename(columns={'cust_id': 'customer_ref'})
    
    if 'customer_ref' not in financial.columns:
        if 'cust_num' in financial.columns:
            financial = financial.rename(columns={'cust_num': 'customer_ref'})
        elif 'cust_id' in financial.columns:
            financial = financial.rename(columns={'cust_id': 'customer_ref'})
    
    if 'customer_ref' not in geographic.columns:
        if 'id' in geographic.columns:
            geographic = geographic.rename(columns={'id': 'customer_ref'})
        elif 'cust_id' in geographic.columns:
            geographic = geographic.rename(columns={'cust_id': 'customer_ref'})
    
    # Merge all sources
    df = df.merge(demo, on='customer_ref', how='left', suffixes=('', '_demo'))
    df = df.merge(credit, on='customer_ref', how='left', suffixes=('', '_credit'))
    df = df.merge(financial, on='customer_ref', how='left', suffixes=('', '_fin'))
    df = df.merge(geographic, on='customer_ref', how='left', suffixes=('', '_geo'))
    
    print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ customer_ref –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    customer_ids = df['customer_ref'].copy()
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    drop_cols = ['customer_ref', 'default_flag', 'default', 'target', 'label']
    drop_cols = [col for col in drop_cols if col in df.columns]
    if drop_cols:
        df = df.drop(columns=drop_cols)
        print(f"  –£–¥–∞–ª–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {drop_cols}")
    
    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    print("\nüîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤...")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].median(), inplace=True)
    
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown', inplace=True)
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏
    df = df.select_dtypes(include=[np.number])
    
    print(f"‚úÖ –ü—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã, –æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –º–æ–¥–µ–ª–∏
    print("\nüéØ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
    if selected_features is not None:
        expected_features = selected_features
        print(f"  –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è selected_features –∏–∑ –º–æ–¥–µ–ª–∏")
    elif feature_cols is not None:
        expected_features = feature_cols
        print(f"  –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è feature_cols –∏–∑ –º–æ–¥–µ–ª–∏")
    else:
        # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –º–æ–¥–µ–ª–∏
        try:
            if hasattr(model, 'feature_names_in_'):
                expected_features = model.feature_names_in_
            elif hasattr(model, 'feature_name_'):
                expected_features = model.feature_name_
            else:
                print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –º–æ–¥–µ–ª–∏")
                print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏...")
                expected_features = df.select_dtypes(include=[np.number]).columns.tolist()
        except:
            expected_features = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    missing_features = set(expected_features) - set(df.columns)
    if missing_features:
        print(f"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(missing_features)}): {list(missing_features)[:5]}...")
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º 0
        for feat in missing_features:
            df[feat] = 0
        print("  –°–æ–∑–¥–∞–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    X_test = df[expected_features].copy()
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º scaler –µ—Å–ª–∏ –µ—Å—Ç—å
    if scaler is not None:
        print(f"  –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (scaler)...")
        X_test = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )
    
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(expected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(X_test):,} –∫–ª–∏–µ–Ω—Ç–æ–≤")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    print("\nü§ñ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
        if 'models' in model_dict and model_dict['models']:
            base_models = model_dict['models']
            print(f"  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–µ–∫–∏–Ω–≥ –∏–∑ {len(base_models)} –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
            base_predictions = []
            for i, base_model in enumerate(base_models):
                if hasattr(base_model, 'predict_proba'):
                    preds = base_model.predict_proba(X_test)[:, 1]
                else:
                    preds = base_model.predict(X_test)
                base_predictions.append(preds)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏
            X_meta = np.column_stack(base_predictions)
            print(f"  –ú–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏: {X_meta.shape}")
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª—å—é
            if hasattr(model, 'predict_proba'):
                probabilities = model.predict_proba(X_meta)[:, 1]
            else:
                probabilities = model.predict(X_meta)
        else:
            # –ü—Ä—è–º–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            if hasattr(model, 'predict_proba'):
                probabilities = model.predict_proba(X_test)[:, 1]
            else:
                probabilities = model.predict(X_test)
        
        print(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –¥–ª—è {len(probabilities):,} –∫–ª–∏–µ–Ω—Ç–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìù –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    results = pd.DataFrame({
        'customer_ref': customer_ids,
        'probability': probabilities
    })
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–æ—Ç –≤—ã—Å–æ–∫–æ–π –∫ –Ω–∏–∑–∫–æ–π)
    results = results.sort_values('probability', ascending=False).reset_index(drop=True)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_file = test_dir / 'predictions.csv'
    results.to_csv(output_file, index=False)
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 80)
    print("üìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 80)
    
    print(f"\nüéØ –í—Å–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {len(results):,}")
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–µ—Ñ–æ–ª—Ç–∞:")
    print(f"  –ú–∏–Ω–∏–º—É–º:     {results['probability'].min():.6f}")
    print(f"  –ú–∞–∫—Å–∏–º—É–º:    {results['probability'].max():.6f}")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ:     {results['probability'].mean():.6f}")
    print(f"  –ú–µ–¥–∏–∞–Ω–∞:     {results['probability'].median():.6f}")
    print(f"  Std:         {results['probability'].std():.6f}")
    
    print(f"\nüìä –ö–≤–∞–Ω—Ç–∏–ª–∏:")
    for q in [0.25, 0.50, 0.75, 0.90, 0.95, 0.99]:
        val = results['probability'].quantile(q)
        print(f"  {int(q*100):2d}%:          {val:.6f}")
    
    print(f"\nüé≤ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–æ—Ä–æ–≥–∞–º:")
    print(f"{'–ü–æ—Ä–æ–≥':<12} {'–î–µ—Ñ–æ–ª—Ç–æ–≤':<12} {'–ü—Ä–æ—Ü–µ–Ω—Ç':<12}")
    print("-" * 40)
    
    thresholds = [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.50]
    for threshold in thresholds:
        count = (results['probability'] >= threshold).sum()
        pct = count / len(results) * 100
        print(f"{threshold:<12.4f} {count:<12,} {pct:<12.2f}%")
    
    # –¢–æ–ø-20 —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
    print(f"\nüî¥ –¢–û–ü-20 –°–ê–ú–´–• –†–ò–°–ö–û–í–ê–ù–ù–´–• –ö–õ–ò–ï–ù–¢–û–í:")
    print(f"{'Rank':<6} {'Customer ID':<15} {'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å':<15}")
    print("-" * 40)
    for idx, row in results.head(20).iterrows():
        print(f"{idx+1:<6} {row['customer_ref']:<15} {row['probability']:<15.6f}")
    
    # –¢–æ–ø-20 —Å–∞–º—ã—Ö –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
    print(f"\nüü¢ –¢–û–ü-20 –°–ê–ú–´–• –ë–ï–ó–û–ü–ê–°–ù–´–• –ö–õ–ò–ï–ù–¢–û–í:")
    print(f"{'Rank':<6} {'Customer ID':<15} {'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å':<15}")
    print("-" * 40)
    for idx, row in results.tail(20).iterrows():
        rank = len(results) - idx
        print(f"{rank:<6} {row['customer_ref']:<15} {row['probability']:<15.6f}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n" + "=" * 80)
    print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ")
    print("=" * 80)
    
    median_prob = results['probability'].median()
    mean_prob = results['probability'].mean()
    
    if mean_prob < 0.10:
        recommended_threshold = 0.05
    elif mean_prob < 0.20:
        recommended_threshold = 0.10
    else:
        recommended_threshold = 0.20
    
    high_risk = (results['probability'] >= recommended_threshold).sum()
    high_risk_pct = high_risk / len(results) * 100
    
    print(f"\n1Ô∏è‚É£ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –∑–∞—è–≤–æ–∫: {recommended_threshold:.4f}")
    print(f"   –ü—Ä–∏ —ç—Ç–æ–º –ø–æ—Ä–æ–≥–µ –±—É–¥–µ—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω–æ: {high_risk:,} –∑–∞—è–≤–æ–∫ ({high_risk_pct:.2f}%)")
    
    print(f"\n2Ô∏è‚É£ –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–µ—Ñ–æ–ª—Ç–∞ –≤ –ø–æ—Ä—Ç—Ñ–µ–ª–µ: {mean_prob:.4f} ({mean_prob*100:.2f}%)")
    
    if mean_prob > 0.15:
        print("   ‚ö†Ô∏è  –í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ –≤ –ø–æ—Ä—Ç—Ñ–µ–ª–µ!")
    elif mean_prob > 0.10:
        print("   ‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞")
    else:
        print("   ‚úÖ –ü—Ä–∏–µ–º–ª–µ–º—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞")
    
    print(f"\n3Ô∏è‚É£ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞:")
    low_risk = (results['probability'] < 0.10).sum()
    medium_risk = ((results['probability'] >= 0.10) & (results['probability'] < 0.30)).sum()
    high_risk_final = (results['probability'] >= 0.30).sum()
    
    print(f"   –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫ (<10%):     {low_risk:,} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({low_risk/len(results)*100:.1f}%)")
    print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ (10-30%):  {medium_risk:,} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({medium_risk/len(results)*100:.1f}%)")
    print(f"   –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ (>30%):    {high_risk_final:,} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({high_risk_final/len(results)*100:.1f}%)")
    
    print("\n" + "=" * 80)
    print("‚úÖ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 80)
    print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")
    print(f"üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results):,} –∫–ª–∏–µ–Ω—Ç–æ–≤")
    print()

if __name__ == '__main__':
    main()
