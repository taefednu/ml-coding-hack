{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1827c3",
   "metadata": {},
   "source": [
    "# End-to-end walkthrough: credit scoring pipeline\n",
    "\n",
    "Эта тетрадь пошагово повторяет действующий пайплайн скора, опираясь только на готовые функции проекта: от загрузки данных до explainability артефактов.\n",
    "\n",
    "## План\n",
    "1. Настройка окружения и импорт модулей.\n",
    "2. Разбор `configs/default.yaml` и ключевых гиперпараметров.\n",
    "3. Ingest + master-table и первичный EDA.\n",
    "4. Временной сплит train/valid/OOT.\n",
    "5. Feature engineering (`build_features`) и визуализации.\n",
    "6. Подготовка матриц X/y и отбор признаков.\n",
    "7. Обучение Champion/Challenger моделей.\n",
    "8. Метрики качества + ROC/PR/калибровка/лифт.\n",
    "9. Explainability: коэффициенты, reason codes, SHAP/feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37774c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проект: /home/zerotwo/ml-coding-hack\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"configs\").exists():\n",
    "    for parent in PROJECT_ROOT.parents:\n",
    "        if (parent / \"configs\").exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "    else:\n",
    "        raise RuntimeError(\"Не удалось найти корень проекта с папкой configs/\")\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.utils import load_config, seed_everything\n",
    "from src.data_loading import load_master_dataset, build_credit_history_features\n",
    "from src.modeling import (\n",
    "    split_by_time,\n",
    "    train_logistic_woe,\n",
    "    train_catboost,\n",
    "    train_lgbm,\n",
    ")\n",
    "from scripts.train import (\n",
    "    _augment_with_features,\n",
    "    _select_feature_columns,\n",
    "    _align_frames,\n",
    "    _run_feature_diagnostics,\n",
    "    _drop_sensitive,\n",
    ")\n",
    "from src.metrics import (\n",
    "    compute_metrics,\n",
    "    report_to_dict,\n",
    "    reliability_curve,\n",
    "    lift_curve,\n",
    ")\n",
    "from src.explainability import champion_coefficients\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "pd.options.display.max_columns = 120\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "print(f\"Проект: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1ea84",
   "metadata": {},
   "source": [
    "## Шаг 2. Конфигурация пайплайна\n",
    "Разбираем `configs/default.yaml`: источники данных, таргет, временные границы, параметры feature engineering и моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca86885",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 17) (195548128.py, line 17)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 17)\n"
     ]
    }
   ],
   "source": [
    "config_path = PROJECT_ROOT / \"configs\" / \"default.yaml\"\n",
    "config = load_config(config_path)\n",
    "seed_everything(config.get(\"seed\", 42))\n",
    "print(f\"Используем конфиг: {config_path}\")\n",
    "\n",
    "sections = {\n",
    "    \"paths\": config.get(\"paths\", {}),\n",
    "    \"data_sources\": config.get(\"data_sources\", {}),\n",
    "    \"target\": config.get(\"target\", {}),\n",
    "    \"split\": config.get(\"split\", {}),\n",
    "    \"feature_engineering\": config.get(\"feature_engineering\", {}),\n",
    "    \"modeling\": config.get(\"modeling\", {}),\n",
    "    \"feature_selection\": config.get(\"feature_selection\", {}),\n",
    "    \"calibration\": config.get(\"calibration\", {}),\n",
    "}\n",
    "for name, payload in sections.items():\n",
    "    print(\"====\", name.upper(), \"====\")\n",
    "    print(yaml.safe_dump(payload, sort_keys=False, allow_unicode=True))\n",
    "\n",
    "artifacts_dir = (PROJECT_ROOT / config[\"paths\"][\"artifacts_dir\"]).resolve()\n",
    "models_dir = (PROJECT_ROOT / config[\"paths\"][\"models_dir\"]).resolve()\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1556f32",
   "metadata": {},
   "source": [
    "## Шаг 3. Ingest и master-table\n",
    "Построим master-table из всех источников, добавим агрегаты по кредитной истории и посмотрим базовый EDA: размеры, head(), таргет, пропуски и распределения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29341142",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m target_col = \u001b[43mconfig\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m date_col = config[\u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mdate_column\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m id_col = config.get(\u001b[33m\"\u001b[39m\u001b[33mmerging\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33mid_col\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcustomer_ref\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "target_col = config[\"target\"][\"column\"]\n",
    "date_col = config[\"split\"][\"date_column\"]\n",
    "id_col = config.get(\"merging\", {}).get(\"id_col\", \"customer_ref\")\n",
    "app_col = config[\"split\"].get(\"application_id_col\", \"application_id\")\n",
    "\n",
    "master_df, credit_history = load_master_dataset(config)\n",
    "print(f\"Master-table после мерджа справочников: {master_df.shape}\")\n",
    "if credit_history is not None and not credit_history.empty:\n",
    "    credit_features = build_credit_history_features(credit_history, master_df, config)\n",
    "    master_df = master_df.merge(credit_features, on=app_col, how=\"left\")\n",
    "    print(f\"Добавили агрегаты кредитной истории: {master_df.shape}\")\n",
    "\n",
    "master_df[date_col] = pd.to_datetime(master_df[date_col], errors=\"coerce\")\n",
    "master_df = master_df.dropna(subset=[target_col, date_col]).sort_values(date_col).reset_index(drop=True)\n",
    "print(f\"После фильтра по {target_col}/{date_col}: {master_df.shape}\")\n",
    "\n",
    "display(master_df.head())\n",
    "\n",
    "print(\"Распределение таргета (count/share):\")\n",
    "target_stats = master_df[target_col].value_counts(dropna=False).to_frame(\"count\")\n",
    "target_stats[\"share\"] = target_stats[\"count\"] / len(master_df)\n",
    "display(target_stats)\n",
    "\n",
    "key_cols = [id_col, app_col, target_col, date_col, \"loan_amount\", \"annual_income\", \"credit_utilization\", \"debt_to_income_ratio\"]\n",
    "missing_info = {\n",
    "    col: int(master_df[col].isna().sum())\n",
    "    for col in key_cols\n",
    "    if col in master_df.columns\n",
    "}\n",
    "display(pd.DataFrame.from_dict(missing_info, orient=\"index\", columns=[\"missing_rows\"]))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.countplot(x=target_col, data=master_df, ax=axes[0])\n",
    "axes[0].set_title(\"Таргет 0/1\")\n",
    "axes[0].set_xlabel(\"default\")\n",
    "monthly = master_df.set_index(date_col).resample(\"M\").size()\n",
    "axes[1].plot(monthly.index, monthly.values)\n",
    "axes[1].set_title(\"Количество заявок по месяцам\")\n",
    "axes[1].set_ylabel(\"# заявок\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "feature_candidates = [\"loan_amount\", \"annual_income\", \"credit_utilization\", \"debt_to_income_ratio\"]\n",
    "available = [col for col in feature_candidates if col in master_df.columns]\n",
    "if available:\n",
    "    sample = master_df[available].sample(min(len(master_df), 5000), random_state=42)\n",
    "    fig, axes = plt.subplots(1, len(available), figsize=(4 * len(available), 3))\n",
    "    if len(available) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, col in zip(axes, available):\n",
    "        sns.histplot(sample[col], bins=30, ax=ax, kde=False)\n",
    "        ax.set_title(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b658b7",
   "metadata": {},
   "source": [
    "## Шаг 4. Временной сплит\n",
    "Используем `split_by_time` из `src.modeling` и границы из конфига, чтобы получить train/valid/OOT. Проверим размеры, диапазоны дат и долю дефолтов для каждого среза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = config[\"split\"][\"train_end\"]\n",
    "valid_end = config[\"split\"][\"valid_end\"]\n",
    "raw_splits = split_by_time(master_df, date_col, train_end, valid_end)\n",
    "\n",
    "summary = []\n",
    "for name, frame in raw_splits.items():\n",
    "    if frame.empty:\n",
    "        continue\n",
    "    date_range = frame[date_col].agg([\"min\", \"max\"])\n",
    "    summary.append(\n",
    "        {\n",
    "            \"split\": name,\n",
    "            \"rows\": len(frame),\n",
    "            \"date_start\": date_range[\"min\"],\n",
    "            \"date_end\": date_range[\"max\"],\n",
    "            \"default_rate\": frame[target_col].mean(),\n",
    "        }\n",
    "    )\n",
    "display(pd.DataFrame(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a2147",
   "metadata": {},
   "source": [
    "## Шаг 5. Feature engineering\n",
    "Воспользуемся `_augment_with_features`, который внутри вызывает `build_features` и добавляет ratio/rolling/trend/behavioral признаки. Покажем, сколько новых фич появилось и как они распределены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e82d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cfg = config.get(\"feature_engineering\", {})\n",
    "client_id_col = config[\"split\"].get(\"client_id_col\")\n",
    "MAX_ROWS_PER_SPLIT = None  # при необходимости можно ограничить размер срезов для быстрой отладки\n",
    "\n",
    "processed_splits = {}\n",
    "for name, frame in raw_splits.items():\n",
    "    subset = frame.copy()\n",
    "    if MAX_ROWS_PER_SPLIT:\n",
    "        subset = subset.iloc[: min(MAX_ROWS_PER_SPLIT, len(subset))].copy()\n",
    "    processed_splits[name] = subset\n",
    "\n",
    "augmented_splits = {}\n",
    "feature_stats = []\n",
    "for name, frame in processed_splits.items():\n",
    "    augmented = _augment_with_features(frame, feature_cfg, date_col, client_id_col)\n",
    "    augmented_splits[name] = augmented\n",
    "    engineered_cols = [col for col in augmented.columns if col not in frame.columns]\n",
    "    feature_stats.append(\n",
    "        {\n",
    "            \"split\": name,\n",
    "            \"rows\": len(augmented),\n",
    "            \"baseline_cols\": len(frame.columns),\n",
    "            \"engineered_cols\": len(engineered_cols),\n",
    "            \"total_cols\": len(augmented.columns),\n",
    "        }\n",
    "    )\n",
    "display(pd.DataFrame(feature_stats))\n",
    "\n",
    "train_engineered = [col for col in augmented_splits[\"train\"].columns if col not in raw_splits[\"train\"].columns]\n",
    "print(f\"Новых признаков в train: {len(train_engineered)}\")\n",
    "display(augmented_splits[\"train\"][train_engineered[:10]].head())\n",
    "\n",
    "stat_cols = train_engineered[:8]\n",
    "if stat_cols:\n",
    "    display(augmented_splits[\"train\"][stat_cols].describe().T)\n",
    "\n",
    "feature_strength_path = artifacts_dir / \"feature_strength.json\"\n",
    "feature_strength = {}\n",
    "if feature_strength_path.exists():\n",
    "    with open(feature_strength_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        feature_strength = json.load(fh)\n",
    "    top_auc_cols = [item[\"feature\"] for item in feature_strength.get(\"top_auc\", []) if item[\"feature\"] in augmented_splits[\"train\"].columns][:6]\n",
    "else:\n",
    "    top_auc_cols = train_engineered[:6]\n",
    "\n",
    "if top_auc_cols:\n",
    "    corr_sample = augmented_splits[\"train\"][top_auc_cols].sample(min(len(augmented_splits[\"train\"]), 5000), random_state=42)\n",
    "    corr = corr_sample.corr()\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(corr, annot=True, cmap=\"crest\", fmt=\".2f\")\n",
    "    plt.title(\"Корреляции топовых engineered фич\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b97be",
   "metadata": {},
   "source": [
    "## Шаг 6. Матрицы X/y и отбор фич\n",
    "Повторяем продакшн-логику: фильтруем потенциально утечные признаки, выравниваем срезы, запускаем `_run_feature_diagnostics`, убираем подозрительные и чувствительные столбцы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceaa8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_like = config.get(\"merging\", {}).get(\"id_like_cols\", [])\n",
    "forbidden_ids = [col for col in [id_col, app_col] if col]\n",
    "forbidden_ids.extend([col for col in id_like if col])\n",
    "forbidden_ids = list(dict.fromkeys(forbidden_ids))\n",
    "\n",
    "safe_columns = _select_feature_columns(augmented_splits, target_col, date_col, forbidden_ids)\n",
    "aligned_splits = _align_frames(augmented_splits, safe_columns)\n",
    "\n",
    "diagnostics = _run_feature_diagnostics(\n",
    "    aligned_splits[\"train\"],\n",
    "    aligned_splits[\"valid\"],\n",
    "    target_col,\n",
    "    date_col,\n",
    "    artifacts_dir,\n",
    "    config.get(\"feature_selection\", {}),\n",
    ")\n",
    "drop_candidates = diagnostics.get(\"drop_columns\", [])\n",
    "if drop_candidates:\n",
    "    for name in aligned_splits:\n",
    "        aligned_splits[name] = aligned_splits[name].drop(columns=drop_candidates, errors=\"ignore\")\n",
    "\n",
    "sensitive_cols = config.get(\"fairness\", {}).get(\"sensitive_cols\", [])\n",
    "if sensitive_cols:\n",
    "    for name in aligned_splits:\n",
    "        aligned_splits[name] = _drop_sensitive(aligned_splits[name], sensitive_cols)\n",
    "\n",
    "train_df = aligned_splits[\"train\"].copy()\n",
    "valid_df = aligned_splits[\"valid\"].copy()\n",
    "oot_df = aligned_splits[\"oot\"].copy()\n",
    "\n",
    "model_features = [col for col in train_df.columns if col != target_col]\n",
    "print(f\"Финальный train shape: {train_df.shape}, число фич без таргета: {len(model_features)}\")\n",
    "\n",
    "if feature_strength:\n",
    "    top_auc = pd.DataFrame(feature_strength.get(\"top_auc\", [])[:10])\n",
    "    display(top_auc)\n",
    "\n",
    "print(\"Диагностика single-feature:\")\n",
    "print(json.dumps({k: diagnostics.get(k) for k in [\"suspicious\", \"low_variance\", \"weak_auc\"]}, indent=2)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981f00c",
   "metadata": {},
   "source": [
    "## Шаг 7. Champion / Challenger\n",
    "Обучаем логистическую регрессию с WOE (champion) и деревья (CatBoost, LightGBM) теми же функциями, что и продакшн. Сравниваем валидационные метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e67e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = config.get(\"modeling\", {})\n",
    "cv_folds = model_cfg.get(\"cv_folds\", 1)\n",
    "group_col = client_id_col if client_id_col and client_id_col in train_df.columns else None\n",
    "\n",
    "champion_model = train_logistic_woe(train_df, valid_df, target_col, model_cfg.get(\"logistic\", {}))\n",
    "cat_model = train_catboost(\n",
    "    train_df,\n",
    "    valid_df,\n",
    "    target_col,\n",
    "    model_cfg.get(\"catboost\", {}),\n",
    "    date_col,\n",
    "    group_col,\n",
    "    cv_folds,\n",
    ")\n",
    "lgbm_model = train_lgbm(\n",
    "    train_df,\n",
    "    valid_df,\n",
    "    target_col,\n",
    "    model_cfg.get(\"lightgbm\", {}),\n",
    "    date_col,\n",
    "    group_col,\n",
    "    cv_folds,\n",
    ")\n",
    "\n",
    "trained_models = [m for m in [champion_model, cat_model, lgbm_model] if m is not None]\n",
    "metrics_table = pd.DataFrame(\n",
    "    [\n",
    "        {\"model\": m.name, **m.metrics}\n",
    "        for m in trained_models\n",
    "    ]\n",
    ")\n",
    "display(metrics_table)\n",
    "\n",
    "challengers = [m for m in trained_models if m.name != \"logistic_woe\"]\n",
    "best_challenger = max(challengers, key=lambda m: m.metrics.get(\"roc_auc\", 0.0)) if challengers else None\n",
    "if best_challenger:\n",
    "    print(f\"Лучший challenger: {best_challenger.name} (ROC-AUC={best_challenger.metrics['roc_auc']:.4f})\")\n",
    "else:\n",
    "    print(\"Челленджеры недоступны (catboost/lightgbm не установлены)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bac7e",
   "metadata": {},
   "source": [
    "## Шаг 8. Метрики и визуализации\n",
    "Считаем ROC/PR/калибровку/лифт по champion и лучшему challenger (если он есть). Параллельно загружаем готовые `artifacts/metrics.json`, чтобы сравнить с тренировочным пайплайном."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02431b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = valid_df[target_col].values\n",
    "oot_y = oot_df[target_col].values\n",
    "\n",
    "predictions = {}\n",
    "eval_rows = []\n",
    "for model in trained_models:\n",
    "    valid_pred = model.predict_proba(valid_df)\n",
    "    oot_pred = model.predict_proba(oot_df)\n",
    "    predictions[model.name] = {\"valid\": valid_pred, \"oot\": oot_pred}\n",
    "    valid_report = report_to_dict(compute_metrics(valid_y, valid_pred))\n",
    "    oot_report = report_to_dict(compute_metrics(oot_y, oot_pred))\n",
    "    eval_rows.append({\"model\": model.name, **{f\"valid_{k}\": v for k, v in valid_report.items()}, **{f\"oot_{k}\": v for k, v in oot_report.items()}})\n",
    "\n",
    "eval_df = pd.DataFrame(eval_rows)\n",
    "display(eval_df)\n",
    "\n",
    "metrics_path = artifacts_dir / \"metrics.json\"\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        stored_metrics = json.load(fh)\n",
    "    print(\"Метрики, сохранённые train-пайплайном:\")\n",
    "    display(stored_metrics)\n",
    "\n",
    "plot_models = [champion_model.name]\n",
    "if best_challenger is not None:\n",
    "    plot_models.append(best_challenger.name)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "for name in plot_models:\n",
    "    fpr, tpr, _ = roc_curve(valid_y, predictions[name][\"valid\"])\n",
    "    auc_value = eval_df.loc[eval_df[\"model\"] == name, \"valid_roc_auc\"].values[0]\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_value:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.title(\"ROC-кривые (valid)\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "for name in plot_models:\n",
    "    precision, recall, _ = precision_recall_curve(valid_y, predictions[name][\"valid\"])\n",
    "    plt.plot(recall, precision, label=name)\n",
    "plt.title(\"PR-кривые (valid)\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "champ_name = champion_model.name\n",
    "rel_valid = reliability_curve(valid_y, predictions[champ_name][\"valid\"], n_bins=10)\n",
    "rel_oot = reliability_curve(oot_y, predictions[champ_name][\"oot\"], n_bins=10)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(rel_valid[\"pred\"], rel_valid[\"true\"], marker=\"o\", label=\"valid\")\n",
    "plt.plot(rel_oot[\"pred\"], rel_oot[\"true\"], marker=\"s\", label=\"oot\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.title(\"Reliability curve: champion\")\n",
    "plt.xlabel(\"Predicted PD\")\n",
    "plt.ylabel(\"Observed PD\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "lift_valid = lift_curve(valid_y, predictions[champ_name][\"valid\"], n_bins=10)\n",
    "lift_df = pd.DataFrame({\"decile\": range(1, len(lift_valid[\"lift\"]) + 1), \"lift\": lift_valid[\"lift\"], \"gain\": lift_valid[\"gain\"]})\n",
    "display(lift_df.head(10))\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(lift_df[\"decile\"], lift_df[\"lift\"], marker=\"o\")\n",
    "plt.title(\"Lift@децили (champion, valid)\")\n",
    "plt.xlabel(\"Дециль\")\n",
    "plt.ylabel(\"Lift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f163db",
   "metadata": {},
   "source": [
    "## Шаг 9. Explainability\n",
    "Используем готовые инструменты: коэффициенты champion (и reason codes из артефакта), а также SHAP/feature importance challenger’а из `artifacts/challenger_shap.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61633932",
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_coefs = pd.DataFrame(champion_coefficients(champion_model))\n",
    "champion_coefs[\"abs_coef\"] = champion_coefs[\"coefficient\"].abs()\n",
    "champion_top = champion_coefs.sort_values(\"abs_coef\", ascending=False).head(15)\n",
    "display(champion_top)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.barplot(data=champion_top, x=\"coefficient\", y=\"feature\", palette=\"viridis\")\n",
    "plt.title(\"Champion: топ коэффициенты (WOE space)\")\n",
    "plt.show()\n",
    "\n",
    "champion_explain_path = artifacts_dir / \"champion_explainability.json\"\n",
    "if champion_explain_path.exists():\n",
    "    with open(champion_explain_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        champion_explain = json.load(fh)\n",
    "    print(\"Reason codes sample:\")\n",
    "    display(champion_explain.get(\"reason_codes\", [])[:5])\n",
    "\n",
    "shap_path = artifacts_dir / \"challenger_shap.json\"\n",
    "if shap_path.exists():\n",
    "    with open(shap_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        shap_payload = json.load(fh)\n",
    "    shap_df = pd.DataFrame(shap_payload.get(\"feature_importance\", [])).head(15)\n",
    "    if not shap_df.empty:\n",
    "        display(shap_df)\n",
    "        value_col = [col for col in shap_df.columns if col != \"feature\"][0]\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.barplot(data=shap_df, x=value_col, y=\"feature\", palette=\"mako\")\n",
    "        plt.title(\"Challenger: SHAP/feature importance\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"SHAP артефакт не найден — возможно, challenger не обучался в предыдущем run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
